{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "mult_deep1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY8A4HBJZKFL",
        "colab_type": "text"
      },
      "source": [
        "#Problem statement\n",
        "We want to teach, with a simple neural network, multiplicative tables from 1 to 10 to our system. Its training data will be built on a list of tuple of three numbers.\n",
        "This is a basic Hello world example and its objective is to start to use Tensorflow 2.0 with Keras API. \n",
        "Thus, let's start to import required libraries specifying to codelab to use version 2 of Tensorflow\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1nHIPsalHWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZQZuYR1acOg",
        "colab_type": "text"
      },
      "source": [
        "We can read data directly from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usbYTysclHWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#local_url='./sample_data/mult_2.csv'\n",
        "git_hub_url=\"https://raw.githubusercontent.com/rosidotidev/HelloWorldDL/master/mult_2.csv\"\n",
        "df = pd.read_csv(git_hub_url, sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrpJBIjplHWz",
        "colab_type": "code",
        "outputId": "ccd690d1-146c-415c-9aaa-d1cfe05811b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>RESULT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     A   B  RESULT\n",
              "0    1   6       6\n",
              "1    1   8       8\n",
              "2    2   4       8\n",
              "3    2   3       6\n",
              "4    2   2       4\n",
              "5    2   1       2\n",
              "6    3   5      15\n",
              "7    3   9      27\n",
              "8    3   4      12\n",
              "9    3   9      27\n",
              "10   3   1       3\n",
              "11   3   3       9\n",
              "12   4   6      24\n",
              "13   4   9      36\n",
              "14   4   4      16\n",
              "15   4   4      16\n",
              "16   5   7      35\n",
              "17   5   3      15\n",
              "18   5   8      40\n",
              "19   5   9      45\n",
              "20   5   8      40\n",
              "21   5   5      25\n",
              "22   6   7      42\n",
              "23   6   9      54\n",
              "24   6   2      12\n",
              "25   7   7      49\n",
              "26   7   9      63\n",
              "27   7   3      21\n",
              "28   7   8      56\n",
              "29   8   1       8\n",
              "30   3  11      33\n",
              "31   4  12      48\n",
              "32   9   6      54\n",
              "33   9   4      36\n",
              "34  10   3      30\n",
              "35  10   6      60\n",
              "36  10   1      10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6H6thi8aqq-",
        "colab_type": "text"
      },
      "source": [
        "Let's split features from results and split again train data from test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU7feVYylHW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop('RESULT',axis=1)\n",
        "y = df['RESULT']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcxLP_17lHXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xulihc2lHXM",
        "colab_type": "code",
        "outputId": "eb0fb1c5-2a1f-41a2-f1c8-f36ecaeed715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=69)\n",
        "X_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     A  B\n",
              "25   7  7\n",
              "21   5  5\n",
              "17   5  3\n",
              "27   7  3\n",
              "4    2  2\n",
              "29   8  1\n",
              "34  10  3\n",
              "13   4  9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLlDaVkWa-WZ",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow likes to work with normalized data, so let's use MinMaxScaler to do that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klpguq6AlHXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train= scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7afGCVoZlHXX",
        "colab_type": "code",
        "outputId": "4c148feb-c67d-48fe-8026-3536ee88745b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgImKQO4lHXa",
        "colab_type": "code",
        "outputId": "7b85cb0c-9eb3-49b9-cfce-6dc58f6bacfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXND4vF5lHXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout,Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUjBc2_HlHXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(2,activation='relu'))\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(Dense(500,activation='relu'))\n",
        "model.add(Dense(30,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVDVIqsDgbO9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MysHyzNlHXl",
        "colab_type": "code",
        "outputId": "d87958be-13b7-4ebc-f87d-7fabc2cf8932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x=X_train,y=y_train.values,\n",
        "          validation_data=(X_test,y_test.values),\n",
        "          batch_size=64,epochs=5000,\n",
        "          callbacks=[early_stop])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 29 samples, validate on 8 samples\n",
            "Epoch 1/5000\n",
            "29/29 [==============================] - 0s 13ms/sample - loss: 1133.8417 - val_loss: 744.3070\n",
            "Epoch 2/5000\n",
            "29/29 [==============================] - 0s 430us/sample - loss: 1133.0791 - val_loss: 743.3632\n",
            "Epoch 3/5000\n",
            "29/29 [==============================] - 0s 424us/sample - loss: 1132.2549 - val_loss: 742.3132\n",
            "Epoch 4/5000\n",
            "29/29 [==============================] - 0s 433us/sample - loss: 1131.3297 - val_loss: 741.1408\n",
            "Epoch 5/5000\n",
            "29/29 [==============================] - 0s 443us/sample - loss: 1130.2902 - val_loss: 739.8783\n",
            "Epoch 6/5000\n",
            "29/29 [==============================] - 0s 410us/sample - loss: 1129.1597 - val_loss: 738.5177\n",
            "Epoch 7/5000\n",
            "29/29 [==============================] - 0s 405us/sample - loss: 1127.9240 - val_loss: 737.0469\n",
            "Epoch 8/5000\n",
            "29/29 [==============================] - 0s 420us/sample - loss: 1126.5764 - val_loss: 735.4567\n",
            "Epoch 9/5000\n",
            "29/29 [==============================] - 0s 412us/sample - loss: 1125.1062 - val_loss: 733.7384\n",
            "Epoch 10/5000\n",
            "29/29 [==============================] - 0s 399us/sample - loss: 1123.5067 - val_loss: 731.8820\n",
            "Epoch 11/5000\n",
            "29/29 [==============================] - 0s 455us/sample - loss: 1121.7706 - val_loss: 729.8744\n",
            "Epoch 12/5000\n",
            "29/29 [==============================] - 0s 457us/sample - loss: 1119.8855 - val_loss: 727.7109\n",
            "Epoch 13/5000\n",
            "29/29 [==============================] - 0s 571us/sample - loss: 1117.8337 - val_loss: 725.3737\n",
            "Epoch 14/5000\n",
            "29/29 [==============================] - 0s 447us/sample - loss: 1115.6086 - val_loss: 722.8541\n",
            "Epoch 15/5000\n",
            "29/29 [==============================] - 0s 386us/sample - loss: 1113.1987 - val_loss: 720.1365\n",
            "Epoch 16/5000\n",
            "29/29 [==============================] - 0s 478us/sample - loss: 1110.5900 - val_loss: 717.2072\n",
            "Epoch 17/5000\n",
            "29/29 [==============================] - 0s 389us/sample - loss: 1107.7693 - val_loss: 714.0535\n",
            "Epoch 18/5000\n",
            "29/29 [==============================] - 0s 485us/sample - loss: 1104.7208 - val_loss: 710.6608\n",
            "Epoch 19/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 1101.4282 - val_loss: 707.0142\n",
            "Epoch 20/5000\n",
            "29/29 [==============================] - 0s 467us/sample - loss: 1097.8635 - val_loss: 703.0875\n",
            "Epoch 21/5000\n",
            "29/29 [==============================] - 0s 686us/sample - loss: 1093.9849 - val_loss: 698.8198\n",
            "Epoch 22/5000\n",
            "29/29 [==============================] - 0s 580us/sample - loss: 1089.7686 - val_loss: 694.2144\n",
            "Epoch 23/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 1085.2037 - val_loss: 689.2668\n",
            "Epoch 24/5000\n",
            "29/29 [==============================] - 0s 539us/sample - loss: 1080.2820 - val_loss: 683.9662\n",
            "Epoch 25/5000\n",
            "29/29 [==============================] - 0s 501us/sample - loss: 1074.9806 - val_loss: 678.2861\n",
            "Epoch 26/5000\n",
            "29/29 [==============================] - 0s 620us/sample - loss: 1069.2786 - val_loss: 672.2145\n",
            "Epoch 27/5000\n",
            "29/29 [==============================] - 0s 816us/sample - loss: 1063.1587 - val_loss: 665.7411\n",
            "Epoch 28/5000\n",
            "29/29 [==============================] - 0s 596us/sample - loss: 1056.5988 - val_loss: 658.8510\n",
            "Epoch 29/5000\n",
            "29/29 [==============================] - 0s 667us/sample - loss: 1049.5734 - val_loss: 651.5260\n",
            "Epoch 30/5000\n",
            "29/29 [==============================] - 0s 568us/sample - loss: 1042.0642 - val_loss: 643.6919\n",
            "Epoch 31/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 1034.0458 - val_loss: 635.3774\n",
            "Epoch 32/5000\n",
            "29/29 [==============================] - 0s 532us/sample - loss: 1025.4712 - val_loss: 626.5767\n",
            "Epoch 33/5000\n",
            "29/29 [==============================] - 0s 391us/sample - loss: 1016.3453 - val_loss: 617.2831\n",
            "Epoch 34/5000\n",
            "29/29 [==============================] - 0s 630us/sample - loss: 1006.6458 - val_loss: 607.4780\n",
            "Epoch 35/5000\n",
            "29/29 [==============================] - 0s 414us/sample - loss: 996.3511 - val_loss: 597.1503\n",
            "Epoch 36/5000\n",
            "29/29 [==============================] - 0s 651us/sample - loss: 985.4402 - val_loss: 586.3109\n",
            "Epoch 37/5000\n",
            "29/29 [==============================] - 0s 410us/sample - loss: 973.8992 - val_loss: 574.9603\n",
            "Epoch 38/5000\n",
            "29/29 [==============================] - 0s 344us/sample - loss: 961.7068 - val_loss: 563.1032\n",
            "Epoch 39/5000\n",
            "29/29 [==============================] - 0s 577us/sample - loss: 948.8503 - val_loss: 550.7482\n",
            "Epoch 40/5000\n",
            "29/29 [==============================] - 0s 502us/sample - loss: 935.3201 - val_loss: 537.9114\n",
            "Epoch 41/5000\n",
            "29/29 [==============================] - 0s 529us/sample - loss: 921.1080 - val_loss: 524.6135\n",
            "Epoch 42/5000\n",
            "29/29 [==============================] - 0s 528us/sample - loss: 906.2090 - val_loss: 510.8816\n",
            "Epoch 43/5000\n",
            "29/29 [==============================] - 0s 674us/sample - loss: 890.6213 - val_loss: 496.7083\n",
            "Epoch 44/5000\n",
            "29/29 [==============================] - 0s 350us/sample - loss: 874.3453 - val_loss: 482.1676\n",
            "Epoch 45/5000\n",
            "29/29 [==============================] - 0s 559us/sample - loss: 857.3887 - val_loss: 467.3172\n",
            "Epoch 46/5000\n",
            "29/29 [==============================] - 0s 402us/sample - loss: 839.7651 - val_loss: 452.2190\n",
            "Epoch 47/5000\n",
            "29/29 [==============================] - 0s 523us/sample - loss: 821.4973 - val_loss: 436.9224\n",
            "Epoch 48/5000\n",
            "29/29 [==============================] - 0s 560us/sample - loss: 802.6122 - val_loss: 421.3156\n",
            "Epoch 49/5000\n",
            "29/29 [==============================] - 0s 538us/sample - loss: 783.1445 - val_loss: 405.6475\n",
            "Epoch 50/5000\n",
            "29/29 [==============================] - 0s 496us/sample - loss: 763.0424 - val_loss: 390.0701\n",
            "Epoch 51/5000\n",
            "29/29 [==============================] - 0s 438us/sample - loss: 742.3810 - val_loss: 374.7162\n",
            "Epoch 52/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 721.2533 - val_loss: 359.7223\n",
            "Epoch 53/5000\n",
            "29/29 [==============================] - 0s 546us/sample - loss: 699.7553 - val_loss: 345.2486\n",
            "Epoch 54/5000\n",
            "29/29 [==============================] - 0s 636us/sample - loss: 677.9740 - val_loss: 331.4717\n",
            "Epoch 55/5000\n",
            "29/29 [==============================] - 0s 473us/sample - loss: 656.0089 - val_loss: 318.5848\n",
            "Epoch 56/5000\n",
            "29/29 [==============================] - 0s 511us/sample - loss: 633.9792 - val_loss: 306.7965\n",
            "Epoch 57/5000\n",
            "29/29 [==============================] - 0s 504us/sample - loss: 612.0141 - val_loss: 296.3290\n",
            "Epoch 58/5000\n",
            "29/29 [==============================] - 0s 576us/sample - loss: 590.2495 - val_loss: 287.4155\n",
            "Epoch 59/5000\n",
            "29/29 [==============================] - 0s 593us/sample - loss: 568.8390 - val_loss: 280.2963\n",
            "Epoch 60/5000\n",
            "29/29 [==============================] - 0s 583us/sample - loss: 547.9482 - val_loss: 275.2130\n",
            "Epoch 61/5000\n",
            "29/29 [==============================] - 0s 540us/sample - loss: 527.7531 - val_loss: 272.3998\n",
            "Epoch 62/5000\n",
            "29/29 [==============================] - 0s 549us/sample - loss: 508.4368 - val_loss: 272.0730\n",
            "Epoch 63/5000\n",
            "29/29 [==============================] - 0s 408us/sample - loss: 490.1855 - val_loss: 274.4169\n",
            "Epoch 64/5000\n",
            "29/29 [==============================] - 0s 459us/sample - loss: 473.1830 - val_loss: 279.5659\n",
            "Epoch 65/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 457.6028 - val_loss: 287.5847\n",
            "Epoch 66/5000\n",
            "29/29 [==============================] - 0s 517us/sample - loss: 443.4087 - val_loss: 298.4731\n",
            "Epoch 67/5000\n",
            "29/29 [==============================] - 0s 469us/sample - loss: 430.7613 - val_loss: 312.1214\n",
            "Epoch 68/5000\n",
            "29/29 [==============================] - 0s 381us/sample - loss: 419.9042 - val_loss: 328.2982\n",
            "Epoch 69/5000\n",
            "29/29 [==============================] - 0s 661us/sample - loss: 410.8897 - val_loss: 346.6422\n",
            "Epoch 70/5000\n",
            "29/29 [==============================] - 0s 451us/sample - loss: 403.6698 - val_loss: 366.6807\n",
            "Epoch 71/5000\n",
            "29/29 [==============================] - 0s 410us/sample - loss: 398.1593 - val_loss: 387.8174\n",
            "Epoch 72/5000\n",
            "29/29 [==============================] - 0s 589us/sample - loss: 394.1016 - val_loss: 409.3400\n",
            "Epoch 73/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 391.4641 - val_loss: 430.4621\n",
            "Epoch 74/5000\n",
            "29/29 [==============================] - 0s 494us/sample - loss: 389.9899 - val_loss: 450.3622\n",
            "Epoch 75/5000\n",
            "29/29 [==============================] - 0s 533us/sample - loss: 389.3761 - val_loss: 468.2474\n",
            "Epoch 76/5000\n",
            "29/29 [==============================] - 0s 626us/sample - loss: 389.2944 - val_loss: 483.4173\n",
            "Epoch 77/5000\n",
            "29/29 [==============================] - 0s 597us/sample - loss: 389.4224 - val_loss: 495.3219\n",
            "Epoch 78/5000\n",
            "29/29 [==============================] - 0s 418us/sample - loss: 389.4725 - val_loss: 503.6030\n",
            "Epoch 79/5000\n",
            "29/29 [==============================] - 0s 421us/sample - loss: 389.2146 - val_loss: 508.1138\n",
            "Epoch 80/5000\n",
            "29/29 [==============================] - 0s 588us/sample - loss: 388.4915 - val_loss: 508.9146\n",
            "Epoch 81/5000\n",
            "29/29 [==============================] - 0s 323us/sample - loss: 387.2237 - val_loss: 506.2480\n",
            "Epoch 82/5000\n",
            "29/29 [==============================] - 0s 435us/sample - loss: 385.4042 - val_loss: 500.5011\n",
            "Epoch 83/5000\n",
            "29/29 [==============================] - 0s 480us/sample - loss: 383.0866 - val_loss: 492.1581\n",
            "Epoch 84/5000\n",
            "29/29 [==============================] - 0s 564us/sample - loss: 380.3672 - val_loss: 481.7549\n",
            "Epoch 85/5000\n",
            "29/29 [==============================] - 0s 319us/sample - loss: 377.3485 - val_loss: 469.8525\n",
            "Epoch 86/5000\n",
            "29/29 [==============================] - 0s 492us/sample - loss: 374.1449 - val_loss: 456.9696\n",
            "Epoch 87/5000\n",
            "29/29 [==============================] - 0s 498us/sample - loss: 370.9114 - val_loss: 443.5775\n",
            "Epoch 88/5000\n",
            "29/29 [==============================] - 0s 504us/sample - loss: 367.7523 - val_loss: 430.0833\n",
            "Epoch 89/5000\n",
            "29/29 [==============================] - 0s 494us/sample - loss: 364.7474 - val_loss: 416.8234\n",
            "Epoch 90/5000\n",
            "29/29 [==============================] - 0s 463us/sample - loss: 361.9531 - val_loss: 404.0637\n",
            "Epoch 91/5000\n",
            "29/29 [==============================] - 0s 491us/sample - loss: 359.4010 - val_loss: 392.0028\n",
            "Epoch 92/5000\n",
            "29/29 [==============================] - 0s 507us/sample - loss: 357.1003 - val_loss: 380.7792\n",
            "Epoch 93/5000\n",
            "29/29 [==============================] - 0s 409us/sample - loss: 355.0420 - val_loss: 370.4798\n",
            "Epoch 94/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 353.2034 - val_loss: 361.1477\n",
            "Epoch 95/5000\n",
            "29/29 [==============================] - 0s 556us/sample - loss: 351.5584 - val_loss: 352.7872\n",
            "Epoch 96/5000\n",
            "29/29 [==============================] - 0s 508us/sample - loss: 350.0854 - val_loss: 345.3881\n",
            "Epoch 97/5000\n",
            "29/29 [==============================] - 0s 485us/sample - loss: 348.7292 - val_loss: 338.9220\n",
            "Epoch 98/5000\n",
            "29/29 [==============================] - 0s 398us/sample - loss: 347.4560 - val_loss: 333.3408\n",
            "Epoch 99/5000\n",
            "29/29 [==============================] - 0s 569us/sample - loss: 346.2328 - val_loss: 328.5921\n",
            "Epoch 100/5000\n",
            "29/29 [==============================] - 0s 600us/sample - loss: 345.0286 - val_loss: 324.6170\n",
            "Epoch 101/5000\n",
            "29/29 [==============================] - 0s 477us/sample - loss: 343.8266 - val_loss: 321.3587\n",
            "Epoch 102/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 342.6122 - val_loss: 318.7595\n",
            "Epoch 103/5000\n",
            "29/29 [==============================] - 0s 572us/sample - loss: 341.3766 - val_loss: 316.7622\n",
            "Epoch 104/5000\n",
            "29/29 [==============================] - 0s 496us/sample - loss: 340.1163 - val_loss: 315.3102\n",
            "Epoch 105/5000\n",
            "29/29 [==============================] - 0s 561us/sample - loss: 338.8322 - val_loss: 314.3476\n",
            "Epoch 106/5000\n",
            "29/29 [==============================] - 0s 630us/sample - loss: 337.5288 - val_loss: 313.8188\n",
            "Epoch 107/5000\n",
            "29/29 [==============================] - 0s 338us/sample - loss: 336.2131 - val_loss: 313.6682\n",
            "Epoch 108/5000\n",
            "29/29 [==============================] - 0s 387us/sample - loss: 334.8935 - val_loss: 313.8401\n",
            "Epoch 109/5000\n",
            "29/29 [==============================] - 0s 395us/sample - loss: 333.5790 - val_loss: 314.2786\n",
            "Epoch 110/5000\n",
            "29/29 [==============================] - 0s 590us/sample - loss: 332.2786 - val_loss: 314.9270\n",
            "Epoch 111/5000\n",
            "29/29 [==============================] - 0s 432us/sample - loss: 331.0000 - val_loss: 315.7287\n",
            "Epoch 112/5000\n",
            "29/29 [==============================] - 0s 630us/sample - loss: 329.7495 - val_loss: 316.6274\n",
            "Epoch 113/5000\n",
            "29/29 [==============================] - 0s 387us/sample - loss: 328.5316 - val_loss: 317.5676\n",
            "Epoch 114/5000\n",
            "29/29 [==============================] - 0s 393us/sample - loss: 327.3490 - val_loss: 318.4958\n",
            "Epoch 115/5000\n",
            "29/29 [==============================] - 0s 642us/sample - loss: 326.2021 - val_loss: 319.3617\n",
            "Epoch 116/5000\n",
            "29/29 [==============================] - 0s 683us/sample - loss: 325.0897 - val_loss: 320.1182\n",
            "Epoch 117/5000\n",
            "29/29 [==============================] - 0s 539us/sample - loss: 324.0091 - val_loss: 320.7239\n",
            "Epoch 118/5000\n",
            "29/29 [==============================] - 0s 606us/sample - loss: 322.9564 - val_loss: 321.1434\n",
            "Epoch 119/5000\n",
            "29/29 [==============================] - 0s 853us/sample - loss: 321.9271 - val_loss: 321.3479\n",
            "Epoch 120/5000\n",
            "29/29 [==============================] - 0s 484us/sample - loss: 320.9163 - val_loss: 321.3167\n",
            "Epoch 121/5000\n",
            "29/29 [==============================] - 0s 481us/sample - loss: 319.9192 - val_loss: 321.0367\n",
            "Epoch 122/5000\n",
            "29/29 [==============================] - 0s 516us/sample - loss: 318.9318 - val_loss: 320.5030\n",
            "Epoch 123/5000\n",
            "29/29 [==============================] - 0s 462us/sample - loss: 317.9504 - val_loss: 319.7182\n",
            "Epoch 124/5000\n",
            "29/29 [==============================] - 0s 483us/sample - loss: 316.9723 - val_loss: 318.6916\n",
            "Epoch 125/5000\n",
            "29/29 [==============================] - 0s 533us/sample - loss: 315.9958 - val_loss: 317.4393\n",
            "Epoch 126/5000\n",
            "29/29 [==============================] - 0s 463us/sample - loss: 315.0205 - val_loss: 315.9839\n",
            "Epoch 127/5000\n",
            "29/29 [==============================] - 0s 559us/sample - loss: 314.0457 - val_loss: 314.3510\n",
            "Epoch 128/5000\n",
            "29/29 [==============================] - 0s 500us/sample - loss: 313.0717 - val_loss: 312.5692\n",
            "Epoch 129/5000\n",
            "29/29 [==============================] - 0s 541us/sample - loss: 312.0996 - val_loss: 310.6682\n",
            "Epoch 130/5000\n",
            "29/29 [==============================] - 0s 398us/sample - loss: 311.1316 - val_loss: 308.6782\n",
            "Epoch 131/5000\n",
            "29/29 [==============================] - 0s 545us/sample - loss: 310.1673 - val_loss: 306.6290\n",
            "Epoch 132/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 309.2193 - val_loss: 304.5422\n",
            "Epoch 133/5000\n",
            "29/29 [==============================] - 0s 500us/sample - loss: 308.2891 - val_loss: 302.4474\n",
            "Epoch 134/5000\n",
            "29/29 [==============================] - 0s 484us/sample - loss: 307.3655 - val_loss: 300.3691\n",
            "Epoch 135/5000\n",
            "29/29 [==============================] - 0s 481us/sample - loss: 306.4489 - val_loss: 298.3300\n",
            "Epoch 136/5000\n",
            "29/29 [==============================] - 0s 482us/sample - loss: 305.5386 - val_loss: 296.3489\n",
            "Epoch 137/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 304.6342 - val_loss: 294.4674\n",
            "Epoch 138/5000\n",
            "29/29 [==============================] - 0s 568us/sample - loss: 303.7379 - val_loss: 292.6839\n",
            "Epoch 139/5000\n",
            "29/29 [==============================] - 0s 528us/sample - loss: 302.8440 - val_loss: 291.0076\n",
            "Epoch 140/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 301.9514 - val_loss: 289.4221\n",
            "Epoch 141/5000\n",
            "29/29 [==============================] - 0s 481us/sample - loss: 301.0634 - val_loss: 287.9368\n",
            "Epoch 142/5000\n",
            "29/29 [==============================] - 0s 522us/sample - loss: 300.1763 - val_loss: 286.5520\n",
            "Epoch 143/5000\n",
            "29/29 [==============================] - 0s 693us/sample - loss: 299.2893 - val_loss: 285.2658\n",
            "Epoch 144/5000\n",
            "29/29 [==============================] - 0s 427us/sample - loss: 298.4017 - val_loss: 284.0739\n",
            "Epoch 145/5000\n",
            "29/29 [==============================] - 0s 524us/sample - loss: 297.5127 - val_loss: 282.9700\n",
            "Epoch 146/5000\n",
            "29/29 [==============================] - 0s 394us/sample - loss: 296.6220 - val_loss: 281.9468\n",
            "Epoch 147/5000\n",
            "29/29 [==============================] - 0s 591us/sample - loss: 295.7292 - val_loss: 280.9949\n",
            "Epoch 148/5000\n",
            "29/29 [==============================] - 0s 611us/sample - loss: 294.8341 - val_loss: 280.1047\n",
            "Epoch 149/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 293.9363 - val_loss: 279.2652\n",
            "Epoch 150/5000\n",
            "29/29 [==============================] - 0s 620us/sample - loss: 293.0358 - val_loss: 278.4653\n",
            "Epoch 151/5000\n",
            "29/29 [==============================] - 0s 599us/sample - loss: 292.1324 - val_loss: 277.6940\n",
            "Epoch 152/5000\n",
            "29/29 [==============================] - 0s 517us/sample - loss: 291.2262 - val_loss: 276.9611\n",
            "Epoch 153/5000\n",
            "29/29 [==============================] - 0s 583us/sample - loss: 290.3164 - val_loss: 276.2331\n",
            "Epoch 154/5000\n",
            "29/29 [==============================] - 0s 560us/sample - loss: 289.4035 - val_loss: 275.4996\n",
            "Epoch 155/5000\n",
            "29/29 [==============================] - 0s 597us/sample - loss: 288.4866 - val_loss: 274.7513\n",
            "Epoch 156/5000\n",
            "29/29 [==============================] - 0s 717us/sample - loss: 287.5653 - val_loss: 273.9804\n",
            "Epoch 157/5000\n",
            "29/29 [==============================] - 0s 528us/sample - loss: 286.6394 - val_loss: 273.1801\n",
            "Epoch 158/5000\n",
            "29/29 [==============================] - 0s 571us/sample - loss: 285.7097 - val_loss: 272.3676\n",
            "Epoch 159/5000\n",
            "29/29 [==============================] - 0s 502us/sample - loss: 284.7723 - val_loss: 271.5351\n",
            "Epoch 160/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 283.8308 - val_loss: 270.6567\n",
            "Epoch 161/5000\n",
            "29/29 [==============================] - 0s 403us/sample - loss: 282.8829 - val_loss: 269.7309\n",
            "Epoch 162/5000\n",
            "29/29 [==============================] - 0s 565us/sample - loss: 281.9279 - val_loss: 268.7578\n",
            "Epoch 163/5000\n",
            "29/29 [==============================] - 0s 595us/sample - loss: 280.9654 - val_loss: 267.7389\n",
            "Epoch 164/5000\n",
            "29/29 [==============================] - 0s 525us/sample - loss: 279.9972 - val_loss: 266.6927\n",
            "Epoch 165/5000\n",
            "29/29 [==============================] - 0s 552us/sample - loss: 279.0195 - val_loss: 265.6187\n",
            "Epoch 166/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 278.0323 - val_loss: 264.5188\n",
            "Epoch 167/5000\n",
            "29/29 [==============================] - 0s 515us/sample - loss: 277.0371 - val_loss: 263.3854\n",
            "Epoch 168/5000\n",
            "29/29 [==============================] - 0s 444us/sample - loss: 276.0337 - val_loss: 262.2233\n",
            "Epoch 169/5000\n",
            "29/29 [==============================] - 0s 556us/sample - loss: 275.0230 - val_loss: 261.0623\n",
            "Epoch 170/5000\n",
            "29/29 [==============================] - 0s 503us/sample - loss: 274.0015 - val_loss: 259.9033\n",
            "Epoch 171/5000\n",
            "29/29 [==============================] - 0s 620us/sample - loss: 272.9681 - val_loss: 258.7473\n",
            "Epoch 172/5000\n",
            "29/29 [==============================] - 0s 590us/sample - loss: 271.9273 - val_loss: 257.5738\n",
            "Epoch 173/5000\n",
            "29/29 [==============================] - 0s 514us/sample - loss: 270.8755 - val_loss: 256.3865\n",
            "Epoch 174/5000\n",
            "29/29 [==============================] - 0s 572us/sample - loss: 269.8125 - val_loss: 255.1891\n",
            "Epoch 175/5000\n",
            "29/29 [==============================] - 0s 529us/sample - loss: 268.7391 - val_loss: 253.9953\n",
            "Epoch 176/5000\n",
            "29/29 [==============================] - 0s 513us/sample - loss: 267.6536 - val_loss: 252.8067\n",
            "Epoch 177/5000\n",
            "29/29 [==============================] - 0s 605us/sample - loss: 266.5549 - val_loss: 251.6424\n",
            "Epoch 178/5000\n",
            "29/29 [==============================] - 0s 500us/sample - loss: 265.4433 - val_loss: 250.4967\n",
            "Epoch 179/5000\n",
            "29/29 [==============================] - 0s 580us/sample - loss: 264.3207 - val_loss: 249.3417\n",
            "Epoch 180/5000\n",
            "29/29 [==============================] - 0s 393us/sample - loss: 263.1851 - val_loss: 248.1768\n",
            "Epoch 181/5000\n",
            "29/29 [==============================] - 0s 467us/sample - loss: 262.0353 - val_loss: 247.0021\n",
            "Epoch 182/5000\n",
            "29/29 [==============================] - 0s 435us/sample - loss: 260.8724 - val_loss: 245.8433\n",
            "Epoch 183/5000\n",
            "29/29 [==============================] - 0s 609us/sample - loss: 259.6946 - val_loss: 244.6956\n",
            "Epoch 184/5000\n",
            "29/29 [==============================] - 0s 404us/sample - loss: 258.5001 - val_loss: 243.5298\n",
            "Epoch 185/5000\n",
            "29/29 [==============================] - 0s 592us/sample - loss: 257.2917 - val_loss: 242.3471\n",
            "Epoch 186/5000\n",
            "29/29 [==============================] - 0s 388us/sample - loss: 256.0677 - val_loss: 241.1659\n",
            "Epoch 187/5000\n",
            "29/29 [==============================] - 0s 564us/sample - loss: 254.8271 - val_loss: 239.9629\n",
            "Epoch 188/5000\n",
            "29/29 [==============================] - 0s 662us/sample - loss: 253.5706 - val_loss: 238.7628\n",
            "Epoch 189/5000\n",
            "29/29 [==============================] - 0s 449us/sample - loss: 252.2958 - val_loss: 237.5352\n",
            "Epoch 190/5000\n",
            "29/29 [==============================] - 0s 529us/sample - loss: 251.0039 - val_loss: 236.2786\n",
            "Epoch 191/5000\n",
            "29/29 [==============================] - 0s 406us/sample - loss: 249.6942 - val_loss: 235.0188\n",
            "Epoch 192/5000\n",
            "29/29 [==============================] - 0s 567us/sample - loss: 248.3675 - val_loss: 233.7566\n",
            "Epoch 193/5000\n",
            "29/29 [==============================] - 0s 550us/sample - loss: 247.0217 - val_loss: 232.4872\n",
            "Epoch 194/5000\n",
            "29/29 [==============================] - 0s 456us/sample - loss: 245.6553 - val_loss: 231.2001\n",
            "Epoch 195/5000\n",
            "29/29 [==============================] - 0s 651us/sample - loss: 244.2720 - val_loss: 229.8980\n",
            "Epoch 196/5000\n",
            "29/29 [==============================] - 0s 553us/sample - loss: 242.8693 - val_loss: 228.5503\n",
            "Epoch 197/5000\n",
            "29/29 [==============================] - 0s 554us/sample - loss: 241.4458 - val_loss: 227.1571\n",
            "Epoch 198/5000\n",
            "29/29 [==============================] - 0s 507us/sample - loss: 240.0021 - val_loss: 225.7532\n",
            "Epoch 199/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 238.5374 - val_loss: 224.3180\n",
            "Epoch 200/5000\n",
            "29/29 [==============================] - 0s 495us/sample - loss: 237.0511 - val_loss: 222.8518\n",
            "Epoch 201/5000\n",
            "29/29 [==============================] - 0s 454us/sample - loss: 235.5420 - val_loss: 221.3829\n",
            "Epoch 202/5000\n",
            "29/29 [==============================] - 0s 484us/sample - loss: 234.0108 - val_loss: 219.8956\n",
            "Epoch 203/5000\n",
            "29/29 [==============================] - 0s 440us/sample - loss: 232.4813 - val_loss: 218.4451\n",
            "Epoch 204/5000\n",
            "29/29 [==============================] - 0s 602us/sample - loss: 230.9289 - val_loss: 217.0567\n",
            "Epoch 205/5000\n",
            "29/29 [==============================] - 0s 443us/sample - loss: 229.3496 - val_loss: 215.7003\n",
            "Epoch 206/5000\n",
            "29/29 [==============================] - 0s 576us/sample - loss: 227.7430 - val_loss: 214.3650\n",
            "Epoch 207/5000\n",
            "29/29 [==============================] - 0s 440us/sample - loss: 226.1094 - val_loss: 213.0356\n",
            "Epoch 208/5000\n",
            "29/29 [==============================] - 0s 771us/sample - loss: 224.4492 - val_loss: 211.6986\n",
            "Epoch 209/5000\n",
            "29/29 [==============================] - 0s 589us/sample - loss: 222.7631 - val_loss: 210.3571\n",
            "Epoch 210/5000\n",
            "29/29 [==============================] - 0s 581us/sample - loss: 221.0511 - val_loss: 208.9820\n",
            "Epoch 211/5000\n",
            "29/29 [==============================] - 0s 566us/sample - loss: 219.3136 - val_loss: 207.5789\n",
            "Epoch 212/5000\n",
            "29/29 [==============================] - 0s 577us/sample - loss: 217.5622 - val_loss: 206.0446\n",
            "Epoch 213/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 215.7893 - val_loss: 204.3690\n",
            "Epoch 214/5000\n",
            "29/29 [==============================] - 0s 552us/sample - loss: 213.9861 - val_loss: 202.5549\n",
            "Epoch 215/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 212.1522 - val_loss: 200.6489\n",
            "Epoch 216/5000\n",
            "29/29 [==============================] - 0s 566us/sample - loss: 210.2842 - val_loss: 198.6545\n",
            "Epoch 217/5000\n",
            "29/29 [==============================] - 0s 488us/sample - loss: 208.3832 - val_loss: 196.5668\n",
            "Epoch 218/5000\n",
            "29/29 [==============================] - 0s 514us/sample - loss: 206.4560 - val_loss: 194.4702\n",
            "Epoch 219/5000\n",
            "29/29 [==============================] - 0s 472us/sample - loss: 204.5101 - val_loss: 192.4171\n",
            "Epoch 220/5000\n",
            "29/29 [==============================] - 0s 576us/sample - loss: 202.5341 - val_loss: 190.3894\n",
            "Epoch 221/5000\n",
            "29/29 [==============================] - 0s 436us/sample - loss: 200.5285 - val_loss: 188.3987\n",
            "Epoch 222/5000\n",
            "29/29 [==============================] - 0s 711us/sample - loss: 198.4948 - val_loss: 186.4373\n",
            "Epoch 223/5000\n",
            "29/29 [==============================] - 0s 598us/sample - loss: 196.4307 - val_loss: 184.4809\n",
            "Epoch 224/5000\n",
            "29/29 [==============================] - 0s 546us/sample - loss: 194.3366 - val_loss: 182.2587\n",
            "Epoch 225/5000\n",
            "29/29 [==============================] - 0s 548us/sample - loss: 192.1764 - val_loss: 180.3726\n",
            "Epoch 226/5000\n",
            "29/29 [==============================] - 0s 570us/sample - loss: 190.0180 - val_loss: 178.5266\n",
            "Epoch 227/5000\n",
            "29/29 [==============================] - 0s 439us/sample - loss: 187.8706 - val_loss: 176.3632\n",
            "Epoch 228/5000\n",
            "29/29 [==============================] - 0s 564us/sample - loss: 185.6228 - val_loss: 173.8522\n",
            "Epoch 229/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 183.2757 - val_loss: 171.4545\n",
            "Epoch 230/5000\n",
            "29/29 [==============================] - 0s 570us/sample - loss: 180.9700 - val_loss: 169.1433\n",
            "Epoch 231/5000\n",
            "29/29 [==============================] - 0s 587us/sample - loss: 178.6187 - val_loss: 166.9026\n",
            "Epoch 232/5000\n",
            "29/29 [==============================] - 0s 528us/sample - loss: 176.2245 - val_loss: 164.7126\n",
            "Epoch 233/5000\n",
            "29/29 [==============================] - 0s 702us/sample - loss: 173.7604 - val_loss: 162.5583\n",
            "Epoch 234/5000\n",
            "29/29 [==============================] - 0s 609us/sample - loss: 171.3142 - val_loss: 159.9597\n",
            "Epoch 235/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 168.6967 - val_loss: 157.3092\n",
            "Epoch 236/5000\n",
            "29/29 [==============================] - 0s 551us/sample - loss: 166.1212 - val_loss: 154.6741\n",
            "Epoch 237/5000\n",
            "29/29 [==============================] - 0s 463us/sample - loss: 163.4987 - val_loss: 152.0581\n",
            "Epoch 238/5000\n",
            "29/29 [==============================] - 0s 368us/sample - loss: 160.8291 - val_loss: 149.4565\n",
            "Epoch 239/5000\n",
            "29/29 [==============================] - 0s 476us/sample - loss: 158.1133 - val_loss: 146.8635\n",
            "Epoch 240/5000\n",
            "29/29 [==============================] - 0s 472us/sample - loss: 155.3516 - val_loss: 144.2673\n",
            "Epoch 241/5000\n",
            "29/29 [==============================] - 0s 623us/sample - loss: 152.5424 - val_loss: 141.6580\n",
            "Epoch 242/5000\n",
            "29/29 [==============================] - 0s 427us/sample - loss: 149.6885 - val_loss: 139.0351\n",
            "Epoch 243/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 146.7998 - val_loss: 136.3201\n",
            "Epoch 244/5000\n",
            "29/29 [==============================] - 0s 541us/sample - loss: 143.9228 - val_loss: 133.4180\n",
            "Epoch 245/5000\n",
            "29/29 [==============================] - 0s 526us/sample - loss: 140.9792 - val_loss: 130.3963\n",
            "Epoch 246/5000\n",
            "29/29 [==============================] - 0s 439us/sample - loss: 138.0131 - val_loss: 127.6674\n",
            "Epoch 247/5000\n",
            "29/29 [==============================] - 0s 502us/sample - loss: 135.1836 - val_loss: 125.0930\n",
            "Epoch 248/5000\n",
            "29/29 [==============================] - 0s 676us/sample - loss: 132.3397 - val_loss: 122.5442\n",
            "Epoch 249/5000\n",
            "29/29 [==============================] - 0s 538us/sample - loss: 129.4449 - val_loss: 120.0986\n",
            "Epoch 250/5000\n",
            "29/29 [==============================] - 0s 550us/sample - loss: 126.5147 - val_loss: 117.7169\n",
            "Epoch 251/5000\n",
            "29/29 [==============================] - 0s 476us/sample - loss: 123.5501 - val_loss: 115.3573\n",
            "Epoch 252/5000\n",
            "29/29 [==============================] - 0s 580us/sample - loss: 120.5776 - val_loss: 112.9120\n",
            "Epoch 253/5000\n",
            "29/29 [==============================] - 0s 496us/sample - loss: 117.6107 - val_loss: 110.1831\n",
            "Epoch 254/5000\n",
            "29/29 [==============================] - 0s 414us/sample - loss: 114.6025 - val_loss: 107.2827\n",
            "Epoch 255/5000\n",
            "29/29 [==============================] - 0s 538us/sample - loss: 111.5549 - val_loss: 104.2645\n",
            "Epoch 256/5000\n",
            "29/29 [==============================] - 0s 337us/sample - loss: 108.5025 - val_loss: 101.1522\n",
            "Epoch 257/5000\n",
            "29/29 [==============================] - 0s 601us/sample - loss: 105.4586 - val_loss: 98.0392\n",
            "Epoch 258/5000\n",
            "29/29 [==============================] - 0s 512us/sample - loss: 102.4075 - val_loss: 94.9489\n",
            "Epoch 259/5000\n",
            "29/29 [==============================] - 0s 611us/sample - loss: 99.3528 - val_loss: 91.9116\n",
            "Epoch 260/5000\n",
            "29/29 [==============================] - 0s 401us/sample - loss: 96.2973 - val_loss: 88.9113\n",
            "Epoch 261/5000\n",
            "29/29 [==============================] - 0s 649us/sample - loss: 93.2645 - val_loss: 85.7927\n",
            "Epoch 262/5000\n",
            "29/29 [==============================] - 0s 442us/sample - loss: 90.2951 - val_loss: 82.6143\n",
            "Epoch 263/5000\n",
            "29/29 [==============================] - 0s 785us/sample - loss: 87.3578 - val_loss: 79.3799\n",
            "Epoch 264/5000\n",
            "29/29 [==============================] - 0s 426us/sample - loss: 84.4259 - val_loss: 76.1062\n",
            "Epoch 265/5000\n",
            "29/29 [==============================] - 0s 541us/sample - loss: 81.5026 - val_loss: 72.8250\n",
            "Epoch 266/5000\n",
            "29/29 [==============================] - 0s 440us/sample - loss: 78.6018 - val_loss: 69.5811\n",
            "Epoch 267/5000\n",
            "29/29 [==============================] - 0s 506us/sample - loss: 75.7532 - val_loss: 66.4830\n",
            "Epoch 268/5000\n",
            "29/29 [==============================] - 0s 401us/sample - loss: 72.9412 - val_loss: 63.5629\n",
            "Epoch 269/5000\n",
            "29/29 [==============================] - 0s 406us/sample - loss: 70.1778 - val_loss: 60.7166\n",
            "Epoch 270/5000\n",
            "29/29 [==============================] - 0s 517us/sample - loss: 67.4540 - val_loss: 58.0205\n",
            "Epoch 271/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 64.7813 - val_loss: 55.4612\n",
            "Epoch 272/5000\n",
            "29/29 [==============================] - 0s 657us/sample - loss: 62.1448 - val_loss: 53.0060\n",
            "Epoch 273/5000\n",
            "29/29 [==============================] - 0s 341us/sample - loss: 59.5472 - val_loss: 50.6173\n",
            "Epoch 274/5000\n",
            "29/29 [==============================] - 0s 560us/sample - loss: 57.0091 - val_loss: 48.1938\n",
            "Epoch 275/5000\n",
            "29/29 [==============================] - 0s 504us/sample - loss: 54.5277 - val_loss: 45.7247\n",
            "Epoch 276/5000\n",
            "29/29 [==============================] - 0s 533us/sample - loss: 52.1010 - val_loss: 43.2101\n",
            "Epoch 277/5000\n",
            "29/29 [==============================] - 0s 489us/sample - loss: 49.7373 - val_loss: 40.7402\n",
            "Epoch 278/5000\n",
            "29/29 [==============================] - 0s 560us/sample - loss: 47.4702 - val_loss: 38.3956\n",
            "Epoch 279/5000\n",
            "29/29 [==============================] - 0s 629us/sample - loss: 45.2783 - val_loss: 36.1606\n",
            "Epoch 280/5000\n",
            "29/29 [==============================] - 0s 362us/sample - loss: 43.1590 - val_loss: 34.0250\n",
            "Epoch 281/5000\n",
            "29/29 [==============================] - 0s 613us/sample - loss: 41.1210 - val_loss: 31.9156\n",
            "Epoch 282/5000\n",
            "29/29 [==============================] - 0s 529us/sample - loss: 39.1636 - val_loss: 29.8481\n",
            "Epoch 283/5000\n",
            "29/29 [==============================] - 0s 550us/sample - loss: 37.2864 - val_loss: 27.8388\n",
            "Epoch 284/5000\n",
            "29/29 [==============================] - 0s 373us/sample - loss: 35.5013 - val_loss: 25.8939\n",
            "Epoch 285/5000\n",
            "29/29 [==============================] - 0s 482us/sample - loss: 33.8129 - val_loss: 24.0399\n",
            "Epoch 286/5000\n",
            "29/29 [==============================] - 0s 470us/sample - loss: 32.2093 - val_loss: 22.2995\n",
            "Epoch 287/5000\n",
            "29/29 [==============================] - 0s 490us/sample - loss: 30.6945 - val_loss: 20.6460\n",
            "Epoch 288/5000\n",
            "29/29 [==============================] - 0s 411us/sample - loss: 29.2892 - val_loss: 19.1965\n",
            "Epoch 289/5000\n",
            "29/29 [==============================] - 0s 595us/sample - loss: 27.9792 - val_loss: 17.8632\n",
            "Epoch 290/5000\n",
            "29/29 [==============================] - 0s 554us/sample - loss: 26.7591 - val_loss: 16.6142\n",
            "Epoch 291/5000\n",
            "29/29 [==============================] - 0s 562us/sample - loss: 25.6291 - val_loss: 15.5066\n",
            "Epoch 292/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 24.5833 - val_loss: 14.5195\n",
            "Epoch 293/5000\n",
            "29/29 [==============================] - 0s 621us/sample - loss: 23.6115 - val_loss: 13.6301\n",
            "Epoch 294/5000\n",
            "29/29 [==============================] - 0s 480us/sample - loss: 22.7118 - val_loss: 12.7962\n",
            "Epoch 295/5000\n",
            "29/29 [==============================] - 0s 496us/sample - loss: 21.8866 - val_loss: 12.0456\n",
            "Epoch 296/5000\n",
            "29/29 [==============================] - 0s 403us/sample - loss: 21.1382 - val_loss: 11.3041\n",
            "Epoch 297/5000\n",
            "29/29 [==============================] - 0s 534us/sample - loss: 20.4482 - val_loss: 10.6350\n",
            "Epoch 298/5000\n",
            "29/29 [==============================] - 0s 501us/sample - loss: 19.8329 - val_loss: 10.0476\n",
            "Epoch 299/5000\n",
            "29/29 [==============================] - 0s 445us/sample - loss: 19.2905 - val_loss: 9.4866\n",
            "Epoch 300/5000\n",
            "29/29 [==============================] - 0s 488us/sample - loss: 18.8363 - val_loss: 8.9484\n",
            "Epoch 301/5000\n",
            "29/29 [==============================] - 0s 414us/sample - loss: 18.4443 - val_loss: 8.4623\n",
            "Epoch 302/5000\n",
            "29/29 [==============================] - 0s 561us/sample - loss: 18.1103 - val_loss: 8.0162\n",
            "Epoch 303/5000\n",
            "29/29 [==============================] - 0s 518us/sample - loss: 17.8524 - val_loss: 7.6627\n",
            "Epoch 304/5000\n",
            "29/29 [==============================] - 0s 470us/sample - loss: 17.6994 - val_loss: 7.4250\n",
            "Epoch 305/5000\n",
            "29/29 [==============================] - 0s 694us/sample - loss: 17.6063 - val_loss: 7.3161\n",
            "Epoch 306/5000\n",
            "29/29 [==============================] - 0s 660us/sample - loss: 17.5329 - val_loss: 7.2805\n",
            "Epoch 307/5000\n",
            "29/29 [==============================] - 0s 464us/sample - loss: 17.4664 - val_loss: 7.3037\n",
            "Epoch 308/5000\n",
            "29/29 [==============================] - 0s 507us/sample - loss: 17.3975 - val_loss: 7.3530\n",
            "Epoch 309/5000\n",
            "29/29 [==============================] - 0s 446us/sample - loss: 17.3220 - val_loss: 7.4394\n",
            "Epoch 310/5000\n",
            "29/29 [==============================] - 0s 441us/sample - loss: 17.2421 - val_loss: 7.5008\n",
            "Epoch 311/5000\n",
            "29/29 [==============================] - 0s 512us/sample - loss: 17.1575 - val_loss: 7.5581\n",
            "Epoch 312/5000\n",
            "29/29 [==============================] - 0s 435us/sample - loss: 17.0553 - val_loss: 7.5744\n",
            "Epoch 313/5000\n",
            "29/29 [==============================] - 0s 552us/sample - loss: 16.9452 - val_loss: 7.5875\n",
            "Epoch 314/5000\n",
            "29/29 [==============================] - 0s 391us/sample - loss: 16.8337 - val_loss: 7.6179\n",
            "Epoch 315/5000\n",
            "29/29 [==============================] - 0s 544us/sample - loss: 16.7258 - val_loss: 7.6771\n",
            "Epoch 316/5000\n",
            "29/29 [==============================] - 0s 485us/sample - loss: 16.6167 - val_loss: 7.7519\n",
            "Epoch 317/5000\n",
            "29/29 [==============================] - 0s 493us/sample - loss: 16.5139 - val_loss: 7.8433\n",
            "Epoch 318/5000\n",
            "29/29 [==============================] - 0s 483us/sample - loss: 16.4100 - val_loss: 7.9553\n",
            "Epoch 319/5000\n",
            "29/29 [==============================] - 0s 464us/sample - loss: 16.3002 - val_loss: 8.0844\n",
            "Epoch 320/5000\n",
            "29/29 [==============================] - 0s 545us/sample - loss: 16.2058 - val_loss: 8.2325\n",
            "Epoch 321/5000\n",
            "29/29 [==============================] - 0s 502us/sample - loss: 16.1131 - val_loss: 8.3911\n",
            "Epoch 322/5000\n",
            "29/29 [==============================] - 0s 513us/sample - loss: 16.0319 - val_loss: 8.5231\n",
            "Epoch 323/5000\n",
            "29/29 [==============================] - 0s 559us/sample - loss: 15.9718 - val_loss: 8.6241\n",
            "Epoch 324/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 15.9170 - val_loss: 8.6863\n",
            "Epoch 325/5000\n",
            "29/29 [==============================] - 0s 552us/sample - loss: 15.8540 - val_loss: 8.7214\n",
            "Epoch 326/5000\n",
            "29/29 [==============================] - 0s 532us/sample - loss: 15.7854 - val_loss: 8.7487\n",
            "Epoch 327/5000\n",
            "29/29 [==============================] - 0s 693us/sample - loss: 15.7115 - val_loss: 8.7776\n",
            "Epoch 328/5000\n",
            "29/29 [==============================] - 0s 429us/sample - loss: 15.6562 - val_loss: 8.8236\n",
            "Epoch 329/5000\n",
            "29/29 [==============================] - 0s 617us/sample - loss: 15.5978 - val_loss: 8.8968\n",
            "Epoch 330/5000\n",
            "29/29 [==============================] - 0s 386us/sample - loss: 15.5308 - val_loss: 8.9970\n",
            "Epoch 331/5000\n",
            "29/29 [==============================] - 0s 434us/sample - loss: 15.4583 - val_loss: 9.1098\n",
            "Epoch 332/5000\n",
            "29/29 [==============================] - 0s 476us/sample - loss: 15.3998 - val_loss: 9.2274\n",
            "Epoch 333/5000\n",
            "29/29 [==============================] - 0s 493us/sample - loss: 15.3594 - val_loss: 9.3492\n",
            "Epoch 334/5000\n",
            "29/29 [==============================] - 0s 370us/sample - loss: 15.3205 - val_loss: 9.4711\n",
            "Epoch 335/5000\n",
            "29/29 [==============================] - 0s 559us/sample - loss: 15.2690 - val_loss: 9.5744\n",
            "Epoch 336/5000\n",
            "29/29 [==============================] - 0s 595us/sample - loss: 15.2143 - val_loss: 9.6307\n",
            "Epoch 337/5000\n",
            "29/29 [==============================] - 0s 517us/sample - loss: 15.1648 - val_loss: 9.6386\n",
            "Epoch 338/5000\n",
            "29/29 [==============================] - 0s 492us/sample - loss: 15.1074 - val_loss: 9.6338\n",
            "Epoch 339/5000\n",
            "29/29 [==============================] - 0s 587us/sample - loss: 15.0465 - val_loss: 9.6323\n",
            "Epoch 340/5000\n",
            "29/29 [==============================] - 0s 411us/sample - loss: 14.9896 - val_loss: 9.6385\n",
            "Epoch 341/5000\n",
            "29/29 [==============================] - 0s 534us/sample - loss: 14.9292 - val_loss: 9.6544\n",
            "Epoch 342/5000\n",
            "29/29 [==============================] - 0s 523us/sample - loss: 14.8668 - val_loss: 9.6955\n",
            "Epoch 343/5000\n",
            "29/29 [==============================] - 0s 505us/sample - loss: 14.8124 - val_loss: 9.7387\n",
            "Epoch 344/5000\n",
            "29/29 [==============================] - 0s 443us/sample - loss: 14.7623 - val_loss: 9.7637\n",
            "Epoch 345/5000\n",
            "29/29 [==============================] - 0s 472us/sample - loss: 14.7124 - val_loss: 9.7783\n",
            "Epoch 346/5000\n",
            "29/29 [==============================] - 0s 560us/sample - loss: 14.6552 - val_loss: 9.7798\n",
            "Epoch 347/5000\n",
            "29/29 [==============================] - 0s 395us/sample - loss: 14.5957 - val_loss: 9.7651\n",
            "Epoch 348/5000\n",
            "29/29 [==============================] - 0s 466us/sample - loss: 14.5382 - val_loss: 9.7487\n",
            "Epoch 349/5000\n",
            "29/29 [==============================] - 0s 536us/sample - loss: 14.4862 - val_loss: 9.7357\n",
            "Epoch 350/5000\n",
            "29/29 [==============================] - 0s 404us/sample - loss: 14.4364 - val_loss: 9.7462\n",
            "Epoch 351/5000\n",
            "29/29 [==============================] - 0s 620us/sample - loss: 14.3765 - val_loss: 9.7781\n",
            "Epoch 352/5000\n",
            "29/29 [==============================] - 0s 510us/sample - loss: 14.3196 - val_loss: 9.8290\n",
            "Epoch 353/5000\n",
            "29/29 [==============================] - 0s 563us/sample - loss: 14.2641 - val_loss: 9.8684\n",
            "Epoch 354/5000\n",
            "29/29 [==============================] - 0s 530us/sample - loss: 14.2065 - val_loss: 9.8614\n",
            "Epoch 355/5000\n",
            "29/29 [==============================] - 0s 647us/sample - loss: 14.1456 - val_loss: 9.7929\n",
            "Epoch 356/5000\n",
            "29/29 [==============================] - 0s 583us/sample - loss: 14.0781 - val_loss: 9.6842\n",
            "Epoch 357/5000\n",
            "29/29 [==============================] - 0s 599us/sample - loss: 14.0160 - val_loss: 9.6093\n",
            "Epoch 358/5000\n",
            "29/29 [==============================] - 0s 455us/sample - loss: 13.9648 - val_loss: 9.5768\n",
            "Epoch 359/5000\n",
            "29/29 [==============================] - 0s 531us/sample - loss: 13.9082 - val_loss: 9.5839\n",
            "Epoch 360/5000\n",
            "29/29 [==============================] - 0s 439us/sample - loss: 13.8430 - val_loss: 9.6068\n",
            "Epoch 361/5000\n",
            "29/29 [==============================] - 0s 454us/sample - loss: 13.7727 - val_loss: 9.6244\n",
            "Epoch 362/5000\n",
            "29/29 [==============================] - 0s 461us/sample - loss: 13.7162 - val_loss: 9.5958\n",
            "Epoch 363/5000\n",
            "29/29 [==============================] - 0s 530us/sample - loss: 13.6590 - val_loss: 9.5213\n",
            "Epoch 364/5000\n",
            "29/29 [==============================] - 0s 512us/sample - loss: 13.5906 - val_loss: 9.4234\n",
            "Epoch 365/5000\n",
            "29/29 [==============================] - 0s 516us/sample - loss: 13.5283 - val_loss: 9.3171\n",
            "Epoch 366/5000\n",
            "29/29 [==============================] - 0s 500us/sample - loss: 13.4629 - val_loss: 9.2304\n",
            "Epoch 367/5000\n",
            "29/29 [==============================] - 0s 360us/sample - loss: 13.3998 - val_loss: 9.1782\n",
            "Epoch 368/5000\n",
            "29/29 [==============================] - 0s 578us/sample - loss: 13.3319 - val_loss: 9.1634\n",
            "Epoch 369/5000\n",
            "29/29 [==============================] - 0s 487us/sample - loss: 13.2672 - val_loss: 9.1715\n",
            "Epoch 370/5000\n",
            "29/29 [==============================] - 0s 568us/sample - loss: 13.2028 - val_loss: 9.1791\n",
            "Epoch 371/5000\n",
            "29/29 [==============================] - 0s 567us/sample - loss: 13.1375 - val_loss: 9.1361\n",
            "Epoch 372/5000\n",
            "29/29 [==============================] - 0s 468us/sample - loss: 13.0668 - val_loss: 9.0648\n",
            "Epoch 373/5000\n",
            "29/29 [==============================] - 0s 436us/sample - loss: 13.0020 - val_loss: 8.9598\n",
            "Epoch 374/5000\n",
            "29/29 [==============================] - 0s 453us/sample - loss: 12.9341 - val_loss: 8.8403\n",
            "Epoch 375/5000\n",
            "29/29 [==============================] - 0s 437us/sample - loss: 12.8679 - val_loss: 8.7290\n",
            "Epoch 376/5000\n",
            "29/29 [==============================] - 0s 516us/sample - loss: 12.8003 - val_loss: 8.6469\n",
            "Epoch 377/5000\n",
            "29/29 [==============================] - 0s 594us/sample - loss: 12.7273 - val_loss: 8.5964\n",
            "Epoch 378/5000\n",
            "29/29 [==============================] - 0s 488us/sample - loss: 12.6584 - val_loss: 8.5622\n",
            "Epoch 379/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 12.5879 - val_loss: 8.5183\n",
            "Epoch 380/5000\n",
            "29/29 [==============================] - 0s 556us/sample - loss: 12.5193 - val_loss: 8.4524\n",
            "Epoch 381/5000\n",
            "29/29 [==============================] - 0s 413us/sample - loss: 12.4484 - val_loss: 8.3711\n",
            "Epoch 382/5000\n",
            "29/29 [==============================] - 0s 548us/sample - loss: 12.3749 - val_loss: 8.2799\n",
            "Epoch 383/5000\n",
            "29/29 [==============================] - 0s 672us/sample - loss: 12.2936 - val_loss: 8.1985\n",
            "Epoch 384/5000\n",
            "29/29 [==============================] - 0s 624us/sample - loss: 12.2162 - val_loss: 8.1131\n",
            "Epoch 385/5000\n",
            "29/29 [==============================] - 0s 490us/sample - loss: 12.1377 - val_loss: 8.0476\n",
            "Epoch 386/5000\n",
            "29/29 [==============================] - 0s 498us/sample - loss: 12.0567 - val_loss: 7.9941\n",
            "Epoch 387/5000\n",
            "29/29 [==============================] - 0s 571us/sample - loss: 11.9729 - val_loss: 7.9350\n",
            "Epoch 388/5000\n",
            "29/29 [==============================] - 0s 583us/sample - loss: 11.8872 - val_loss: 7.8861\n",
            "Epoch 389/5000\n",
            "29/29 [==============================] - 0s 496us/sample - loss: 11.8049 - val_loss: 7.8450\n",
            "Epoch 390/5000\n",
            "29/29 [==============================] - 0s 531us/sample - loss: 11.7176 - val_loss: 7.8036\n",
            "Epoch 391/5000\n",
            "29/29 [==============================] - 0s 547us/sample - loss: 11.6252 - val_loss: 7.7602\n",
            "Epoch 392/5000\n",
            "29/29 [==============================] - 0s 471us/sample - loss: 11.5406 - val_loss: 7.6948\n",
            "Epoch 393/5000\n",
            "29/29 [==============================] - 0s 593us/sample - loss: 11.4506 - val_loss: 7.6179\n",
            "Epoch 394/5000\n",
            "29/29 [==============================] - 0s 582us/sample - loss: 11.3560 - val_loss: 7.5310\n",
            "Epoch 395/5000\n",
            "29/29 [==============================] - 0s 557us/sample - loss: 11.2612 - val_loss: 7.4589\n",
            "Epoch 396/5000\n",
            "29/29 [==============================] - 0s 553us/sample - loss: 11.1664 - val_loss: 7.4158\n",
            "Epoch 397/5000\n",
            "29/29 [==============================] - 0s 527us/sample - loss: 11.0724 - val_loss: 7.3823\n",
            "Epoch 398/5000\n",
            "29/29 [==============================] - 0s 527us/sample - loss: 10.9787 - val_loss: 7.3266\n",
            "Epoch 399/5000\n",
            "29/29 [==============================] - 0s 582us/sample - loss: 10.8878 - val_loss: 7.2693\n",
            "Epoch 400/5000\n",
            "29/29 [==============================] - 0s 664us/sample - loss: 10.7965 - val_loss: 7.2004\n",
            "Epoch 401/5000\n",
            "29/29 [==============================] - 0s 531us/sample - loss: 10.6998 - val_loss: 7.1366\n",
            "Epoch 402/5000\n",
            "29/29 [==============================] - 0s 525us/sample - loss: 10.6066 - val_loss: 7.0930\n",
            "Epoch 403/5000\n",
            "29/29 [==============================] - 0s 673us/sample - loss: 10.5136 - val_loss: 7.0429\n",
            "Epoch 404/5000\n",
            "29/29 [==============================] - 0s 542us/sample - loss: 10.4248 - val_loss: 6.9804\n",
            "Epoch 405/5000\n",
            "29/29 [==============================] - 0s 418us/sample - loss: 10.3309 - val_loss: 6.8989\n",
            "Epoch 406/5000\n",
            "29/29 [==============================] - 0s 487us/sample - loss: 10.2376 - val_loss: 6.8293\n",
            "Epoch 407/5000\n",
            "29/29 [==============================] - 0s 536us/sample - loss: 10.1484 - val_loss: 6.7793\n",
            "Epoch 408/5000\n",
            "29/29 [==============================] - 0s 670us/sample - loss: 10.0579 - val_loss: 6.7339\n",
            "Epoch 409/5000\n",
            "29/29 [==============================] - 0s 792us/sample - loss: 9.9689 - val_loss: 6.6826\n",
            "Epoch 410/5000\n",
            "29/29 [==============================] - 0s 555us/sample - loss: 9.8853 - val_loss: 6.6129\n",
            "Epoch 411/5000\n",
            "29/29 [==============================] - 0s 508us/sample - loss: 9.7947 - val_loss: 6.5307\n",
            "Epoch 412/5000\n",
            "29/29 [==============================] - 0s 570us/sample - loss: 9.7047 - val_loss: 6.4628\n",
            "Epoch 413/5000\n",
            "29/29 [==============================] - 0s 558us/sample - loss: 9.6197 - val_loss: 6.4171\n",
            "Epoch 414/5000\n",
            "29/29 [==============================] - 0s 382us/sample - loss: 9.5337 - val_loss: 6.3722\n",
            "Epoch 415/5000\n",
            "29/29 [==============================] - 0s 533us/sample - loss: 9.4467 - val_loss: 6.3202\n",
            "Epoch 416/5000\n",
            "29/29 [==============================] - 0s 452us/sample - loss: 9.3614 - val_loss: 6.2597\n",
            "Epoch 417/5000\n",
            "29/29 [==============================] - 0s 491us/sample - loss: 9.2809 - val_loss: 6.1947\n",
            "Epoch 418/5000\n",
            "29/29 [==============================] - 0s 557us/sample - loss: 9.1995 - val_loss: 6.1418\n",
            "Epoch 419/5000\n",
            "29/29 [==============================] - 0s 387us/sample - loss: 9.1249 - val_loss: 6.0917\n",
            "Epoch 420/5000\n",
            "29/29 [==============================] - 0s 564us/sample - loss: 9.0508 - val_loss: 6.0423\n",
            "Epoch 421/5000\n",
            "29/29 [==============================] - 0s 651us/sample - loss: 8.9706 - val_loss: 6.0008\n",
            "Epoch 422/5000\n",
            "29/29 [==============================] - 0s 537us/sample - loss: 8.8810 - val_loss: 5.9718\n",
            "Epoch 423/5000\n",
            "29/29 [==============================] - 0s 661us/sample - loss: 8.7893 - val_loss: 5.9580\n",
            "Epoch 424/5000\n",
            "29/29 [==============================] - 0s 543us/sample - loss: 8.6956 - val_loss: 5.9524\n",
            "Epoch 425/5000\n",
            "29/29 [==============================] - 0s 544us/sample - loss: 8.5963 - val_loss: 5.9509\n",
            "Epoch 426/5000\n",
            "29/29 [==============================] - 0s 488us/sample - loss: 8.4990 - val_loss: 5.9470\n",
            "Epoch 427/5000\n",
            "29/29 [==============================] - 0s 390us/sample - loss: 8.3971 - val_loss: 5.9464\n",
            "Epoch 428/5000\n",
            "29/29 [==============================] - 0s 534us/sample - loss: 8.2940 - val_loss: 5.9562\n",
            "Epoch 429/5000\n",
            "29/29 [==============================] - 0s 563us/sample - loss: 8.1901 - val_loss: 5.9762\n",
            "Epoch 430/5000\n",
            "29/29 [==============================] - 0s 438us/sample - loss: 8.0875 - val_loss: 6.0021\n",
            "Epoch 431/5000\n",
            "29/29 [==============================] - 0s 410us/sample - loss: 7.9866 - val_loss: 6.0340\n",
            "Epoch 432/5000\n",
            "29/29 [==============================] - 0s 404us/sample - loss: 7.8822 - val_loss: 6.0687\n",
            "Epoch 433/5000\n",
            "29/29 [==============================] - 0s 530us/sample - loss: 7.7794 - val_loss: 6.0964\n",
            "Epoch 434/5000\n",
            "29/29 [==============================] - 0s 549us/sample - loss: 7.6785 - val_loss: 6.1228\n",
            "Epoch 435/5000\n",
            "29/29 [==============================] - 0s 712us/sample - loss: 7.5838 - val_loss: 6.1658\n",
            "Epoch 436/5000\n",
            "29/29 [==============================] - 0s 482us/sample - loss: 7.4861 - val_loss: 6.2093\n",
            "Epoch 437/5000\n",
            "29/29 [==============================] - 0s 622us/sample - loss: 7.3876 - val_loss: 6.2533\n",
            "Epoch 438/5000\n",
            "29/29 [==============================] - 0s 570us/sample - loss: 7.2929 - val_loss: 6.2894\n",
            "Epoch 439/5000\n",
            "29/29 [==============================] - 0s 452us/sample - loss: 7.2005 - val_loss: 6.3236\n",
            "Epoch 440/5000\n",
            "29/29 [==============================] - 0s 514us/sample - loss: 7.1021 - val_loss: 6.3633\n",
            "Epoch 441/5000\n",
            "29/29 [==============================] - 0s 565us/sample - loss: 7.0006 - val_loss: 6.4098\n",
            "Epoch 442/5000\n",
            "29/29 [==============================] - 0s 430us/sample - loss: 6.9018 - val_loss: 6.4469\n",
            "Epoch 443/5000\n",
            "29/29 [==============================] - 0s 401us/sample - loss: 6.8063 - val_loss: 6.4814\n",
            "Epoch 444/5000\n",
            "29/29 [==============================] - 0s 396us/sample - loss: 6.7155 - val_loss: 6.5243\n",
            "Epoch 445/5000\n",
            "29/29 [==============================] - 0s 476us/sample - loss: 6.6227 - val_loss: 6.5764\n",
            "Epoch 446/5000\n",
            "29/29 [==============================] - 0s 403us/sample - loss: 6.5334 - val_loss: 6.6292\n",
            "Epoch 447/5000\n",
            "29/29 [==============================] - 0s 616us/sample - loss: 6.4440 - val_loss: 6.6652\n",
            "Epoch 448/5000\n",
            "29/29 [==============================] - 0s 541us/sample - loss: 6.3535 - val_loss: 6.7062\n",
            "Epoch 449/5000\n",
            "29/29 [==============================] - 0s 597us/sample - loss: 6.2654 - val_loss: 6.7508\n",
            "Epoch 450/5000\n",
            "29/29 [==============================] - 0s 485us/sample - loss: 6.1760 - val_loss: 6.7828\n",
            "Epoch 451/5000\n",
            "29/29 [==============================] - 0s 489us/sample - loss: 6.0902 - val_loss: 6.8084\n",
            "Epoch 452/5000\n",
            "29/29 [==============================] - 0s 568us/sample - loss: 6.0064 - val_loss: 6.8505\n",
            "Epoch 453/5000\n",
            "29/29 [==============================] - 0s 559us/sample - loss: 5.9214 - val_loss: 6.8831\n",
            "Epoch 454/5000\n",
            "29/29 [==============================] - 0s 438us/sample - loss: 5.8397 - val_loss: 6.9175\n",
            "Epoch 455/5000\n",
            "29/29 [==============================] - 0s 481us/sample - loss: 5.7586 - val_loss: 6.9526\n",
            "Epoch 456/5000\n",
            "29/29 [==============================] - 0s 445us/sample - loss: 5.6779 - val_loss: 6.9869\n",
            "Epoch 457/5000\n",
            "29/29 [==============================] - 0s 391us/sample - loss: 5.5989 - val_loss: 7.0307\n",
            "Epoch 458/5000\n",
            "29/29 [==============================] - 0s 628us/sample - loss: 5.5239 - val_loss: 7.0683\n",
            "Epoch 459/5000\n",
            "29/29 [==============================] - 0s 421us/sample - loss: 5.4467 - val_loss: 7.1086\n",
            "Epoch 460/5000\n",
            "29/29 [==============================] - 0s 595us/sample - loss: 5.3714 - val_loss: 7.1343\n",
            "Epoch 461/5000\n",
            "29/29 [==============================] - 0s 582us/sample - loss: 5.3000 - val_loss: 7.1618\n",
            "Epoch 462/5000\n",
            "29/29 [==============================] - 0s 497us/sample - loss: 5.2229 - val_loss: 7.1788\n",
            "Epoch 463/5000\n",
            "29/29 [==============================] - 0s 591us/sample - loss: 5.1530 - val_loss: 7.2077\n",
            "Epoch 464/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 5.0832 - val_loss: 7.2190\n",
            "Epoch 465/5000\n",
            "29/29 [==============================] - 0s 536us/sample - loss: 5.0133 - val_loss: 7.2349\n",
            "Epoch 466/5000\n",
            "29/29 [==============================] - 0s 491us/sample - loss: 4.9451 - val_loss: 7.2716\n",
            "Epoch 467/5000\n",
            "29/29 [==============================] - 0s 336us/sample - loss: 4.8754 - val_loss: 7.2991\n",
            "Epoch 468/5000\n",
            "29/29 [==============================] - 0s 539us/sample - loss: 4.8088 - val_loss: 7.3138\n",
            "Epoch 469/5000\n",
            "29/29 [==============================] - 0s 411us/sample - loss: 4.7427 - val_loss: 7.3381\n",
            "Epoch 470/5000\n",
            "29/29 [==============================] - 0s 542us/sample - loss: 4.6791 - val_loss: 7.3603\n",
            "Epoch 471/5000\n",
            "29/29 [==============================] - 0s 548us/sample - loss: 4.6141 - val_loss: 7.3770\n",
            "Epoch 472/5000\n",
            "29/29 [==============================] - 0s 507us/sample - loss: 4.5505 - val_loss: 7.4017\n",
            "Epoch 473/5000\n",
            "29/29 [==============================] - 0s 460us/sample - loss: 4.4894 - val_loss: 7.4265\n",
            "Epoch 474/5000\n",
            "29/29 [==============================] - 0s 511us/sample - loss: 4.4294 - val_loss: 7.4522\n",
            "Epoch 475/5000\n",
            "29/29 [==============================] - 0s 623us/sample - loss: 4.3685 - val_loss: 7.4642\n",
            "Epoch 476/5000\n",
            "29/29 [==============================] - 0s 460us/sample - loss: 4.3108 - val_loss: 7.4843\n",
            "Epoch 477/5000\n",
            "29/29 [==============================] - 0s 633us/sample - loss: 4.2519 - val_loss: 7.5239\n",
            "Epoch 478/5000\n",
            "29/29 [==============================] - 0s 513us/sample - loss: 4.1937 - val_loss: 7.5373\n",
            "Epoch 479/5000\n",
            "29/29 [==============================] - 0s 566us/sample - loss: 4.1381 - val_loss: 7.5502\n",
            "Epoch 480/5000\n",
            "29/29 [==============================] - 0s 384us/sample - loss: 4.0809 - val_loss: 7.5706\n",
            "Epoch 481/5000\n",
            "29/29 [==============================] - 0s 461us/sample - loss: 4.0266 - val_loss: 7.5813\n",
            "Epoch 482/5000\n",
            "29/29 [==============================] - 0s 493us/sample - loss: 3.9726 - val_loss: 7.5956\n",
            "Epoch 483/5000\n",
            "29/29 [==============================] - 0s 395us/sample - loss: 3.9178 - val_loss: 7.5990\n",
            "Epoch 484/5000\n",
            "29/29 [==============================] - 0s 532us/sample - loss: 3.8657 - val_loss: 7.6070\n",
            "Epoch 485/5000\n",
            "29/29 [==============================] - 0s 592us/sample - loss: 3.8158 - val_loss: 7.6224\n",
            "Epoch 486/5000\n",
            "29/29 [==============================] - 0s 414us/sample - loss: 3.7664 - val_loss: 7.6291\n",
            "Epoch 487/5000\n",
            "29/29 [==============================] - 0s 440us/sample - loss: 3.7151 - val_loss: 7.6365\n",
            "Epoch 488/5000\n",
            "29/29 [==============================] - 0s 557us/sample - loss: 3.6644 - val_loss: 7.6480\n",
            "Epoch 489/5000\n",
            "29/29 [==============================] - 0s 778us/sample - loss: 3.6179 - val_loss: 7.6625\n",
            "Epoch 490/5000\n",
            "29/29 [==============================] - 0s 536us/sample - loss: 3.5714 - val_loss: 7.6716\n",
            "Epoch 491/5000\n",
            "29/29 [==============================] - 0s 352us/sample - loss: 3.5263 - val_loss: 7.6792\n",
            "Epoch 492/5000\n",
            "29/29 [==============================] - 0s 699us/sample - loss: 3.4820 - val_loss: 7.6915\n",
            "Epoch 493/5000\n",
            "29/29 [==============================] - 0s 538us/sample - loss: 3.4375 - val_loss: 7.7124\n",
            "Epoch 494/5000\n",
            "29/29 [==============================] - 0s 528us/sample - loss: 3.3932 - val_loss: 7.7312\n",
            "Epoch 495/5000\n",
            "29/29 [==============================] - 0s 573us/sample - loss: 3.3471 - val_loss: 7.7404\n",
            "Epoch 496/5000\n",
            "29/29 [==============================] - 0s 568us/sample - loss: 3.3013 - val_loss: 7.7437\n",
            "Epoch 497/5000\n",
            "29/29 [==============================] - 0s 489us/sample - loss: 3.2537 - val_loss: 7.6796\n",
            "Epoch 498/5000\n",
            "29/29 [==============================] - 0s 507us/sample - loss: 3.2120 - val_loss: 7.6092\n",
            "Epoch 499/5000\n",
            "29/29 [==============================] - 0s 535us/sample - loss: 3.1706 - val_loss: 7.5284\n",
            "Epoch 500/5000\n",
            "29/29 [==============================] - 0s 446us/sample - loss: 3.1291 - val_loss: 7.4564\n",
            "Epoch 501/5000\n",
            "29/29 [==============================] - 0s 516us/sample - loss: 3.0879 - val_loss: 7.3950\n",
            "Epoch 502/5000\n",
            "29/29 [==============================] - 0s 385us/sample - loss: 3.0467 - val_loss: 7.3320\n",
            "Epoch 503/5000\n",
            "29/29 [==============================] - 0s 751us/sample - loss: 3.0087 - val_loss: 7.2552\n",
            "Epoch 504/5000\n",
            "29/29 [==============================] - 0s 380us/sample - loss: 2.9681 - val_loss: 7.1643\n",
            "Epoch 505/5000\n",
            "29/29 [==============================] - 0s 475us/sample - loss: 2.9287 - val_loss: 7.0722\n",
            "Epoch 506/5000\n",
            "29/29 [==============================] - 0s 578us/sample - loss: 2.8918 - val_loss: 6.9949\n",
            "Epoch 507/5000\n",
            "29/29 [==============================] - 0s 422us/sample - loss: 2.8576 - val_loss: 6.9386\n",
            "Epoch 508/5000\n",
            "29/29 [==============================] - 0s 519us/sample - loss: 2.8242 - val_loss: 6.9057\n",
            "Epoch 509/5000\n",
            "29/29 [==============================] - 0s 500us/sample - loss: 2.7912 - val_loss: 6.8696\n",
            "Epoch 510/5000\n",
            "29/29 [==============================] - 0s 348us/sample - loss: 2.7607 - val_loss: 6.8416\n",
            "Epoch 511/5000\n",
            "29/29 [==============================] - 0s 417us/sample - loss: 2.7297 - val_loss: 6.7955\n",
            "Epoch 512/5000\n",
            "29/29 [==============================] - 0s 487us/sample - loss: 2.6985 - val_loss: 6.7293\n",
            "Epoch 513/5000\n",
            "29/29 [==============================] - 0s 613us/sample - loss: 2.6660 - val_loss: 6.6688\n",
            "Epoch 514/5000\n",
            "29/29 [==============================] - 0s 624us/sample - loss: 2.6361 - val_loss: 6.6123\n",
            "Epoch 515/5000\n",
            "29/29 [==============================] - 0s 489us/sample - loss: 2.6082 - val_loss: 6.5723\n",
            "Epoch 516/5000\n",
            "29/29 [==============================] - 0s 465us/sample - loss: 2.5782 - val_loss: 6.5371\n",
            "Epoch 517/5000\n",
            "29/29 [==============================] - 0s 733us/sample - loss: 2.5477 - val_loss: 6.4983\n",
            "Epoch 518/5000\n",
            "29/29 [==============================] - 0s 448us/sample - loss: 2.5195 - val_loss: 6.4547\n",
            "Epoch 519/5000\n",
            "29/29 [==============================] - 0s 345us/sample - loss: 2.4930 - val_loss: 6.4182\n",
            "Epoch 520/5000\n",
            "29/29 [==============================] - 0s 538us/sample - loss: 2.4639 - val_loss: 6.3771\n",
            "Epoch 521/5000\n",
            "29/29 [==============================] - 0s 477us/sample - loss: 2.4343 - val_loss: 6.3256\n",
            "Epoch 522/5000\n",
            "29/29 [==============================] - 0s 543us/sample - loss: 2.4081 - val_loss: 6.2830\n",
            "Epoch 523/5000\n",
            "29/29 [==============================] - 0s 454us/sample - loss: 2.3800 - val_loss: 6.2341\n",
            "Epoch 524/5000\n",
            "29/29 [==============================] - 0s 479us/sample - loss: 2.3527 - val_loss: 6.1860\n",
            "Epoch 525/5000\n",
            "29/29 [==============================] - 0s 532us/sample - loss: 2.3266 - val_loss: 6.1488\n",
            "Epoch 526/5000\n",
            "29/29 [==============================] - 0s 472us/sample - loss: 2.3000 - val_loss: 6.1055\n",
            "Epoch 527/5000\n",
            "29/29 [==============================] - 0s 443us/sample - loss: 2.2749 - val_loss: 6.0633\n",
            "Epoch 00527: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fddc62aff98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnkp4oZFlHXq",
        "colab_type": "code",
        "outputId": "e2f94566-cc3c-4365-d89d-ed78f69980a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "losses = pd.DataFrame(model.history.history)\n",
        "losses.plot()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fddc45f6e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXwV1fn48c9zlyyQQBZCEpKQsIQ9\nEjBsIigqiNSK1gUVFa2Vb9WvWrVW29pWW/1pbSulxaV83VsXKLWVuuCCKIKIhEAIyB62hCULhD0k\nuff8/pgJRAyQ5Sb35t7n/Xrd1505c2bmmRCemZw5c0aMMSillAoNDn8HoJRSqvVo0ldKqRCiSV8p\npUKIJn2llAohmvSVUiqEuPwdwOl06tTJZGRk+DsMpZRqU5YvX15mjEmob1lAJ/2MjAxyc3P9HYZS\nSrUpIrLtVMu0eUcppUKIJn2llAohmvSVUiqEBHSbvlIqNFVXV1NUVERlZaW/QwloERERpKam4na7\nG7yOJn2lVMApKioiOjqajIwMRMTf4QQkYwzl5eUUFRXRrVu3Bq+nzTtKqYBTWVlJfHy8JvzTEBHi\n4+Mb/deQJn2lVEDShH9mTfkZBXTSLzl4jK8Ky6ms9vg7FKWUCgoB3aa/50Al1878ijCXg3N6xHN5\ndgpj+yXSPjygw1ZKBYGoqCgOHTrk7zB8LqCzZ7/kDvzhphy+Kizng9W7+cmslbQPc3L9sK7cNqo7\nnTtE+DtEpZRqUwK6ecfpEC7ql8jDl/bji5+NYdbU4VzUL5EXF21h1FMLmPHpRo7VaNOPUqrlGGN4\n4IEHGDBgAFlZWcyaNQuAXbt2MXr0aLKzsxkwYABffPEFHo+Hm2+++XjdadOm+Tn67wroK/26HA5h\nWPd4hnWP5/6xvfn9vHX88aMN/GflTp6dPJheidH+DlEp1QIe/e8avtl5wKfb7NelA7/5fv8G1X37\n7bdZuXIl+fn5lJWVMWTIEEaPHs0bb7zBxRdfzC9/+Us8Hg9Hjhxh5cqVFBcXs3r1agAqKip8Grcv\nBPSV/ql0jW/HM5MH8/ItQ6g4Us3EGYv5b/5Of4ellApCixYt4rrrrsPpdJKYmMh5553HsmXLGDJk\nCC+//DKPPPIIBQUFREdH0717dwoLC7nrrruYN28eHTp08Hf439FmrvTrM6Z3Z967+1zufD2Pu95c\nQfmhY9w8suEPKSilAl9Dr8hb2+jRo1m4cCHvvfceN998M/fddx833XQT+fn5fPjhhzz//PPMnj2b\nl156yd+hfkubvNKvK7FDBP/40TDG9Uvkkf9+wz++OuWIokop1WijRo1i1qxZeDweSktLWbhwIUOH\nDmXbtm0kJiZy22238aMf/Yi8vDzKysrwer1ceeWVPPbYY+Tl5fk7/O9o01f6tSLcTp6dPJipf1/O\nr99ZTXLHCC7sm+jvsJRSQeCKK65gyZIlDBw4EBHhqaeeIikpiVdffZU//OEPuN1uoqKieO211ygu\nLuaWW27B6/UC8MQTT/g5+u8SY4y/YzilnJwc05iXqBypquGavy1he/kR3r1rFF3j27VgdEqplrJ2\n7Vr69u3r7zDahPp+ViKy3BiTU1/9Nt+8U1e7MBfPTT4bgLveWkGNx+vniJRSKrAEVdIHSItrx+NX\nZJG/o4IXF23xdzhKKRVQgi7pA1x6VjIX90/kTx9vYMfeI/4ORymlAkZQJn0R4Tff749D4MkP1vk7\nHKWUChhBmfQBusREcvt5PXmvYBe5W/f6OxyllAoIQZv0AaaO7k6nqDD+/MlGf4eilFIBIaiTfmSY\nkx+f14NFm8pYplf7SikV3EkfYPKwdOLbh/H8Z5v9HYpSKkhFRUWdctnWrVsZMGBAK0ZzekGf9CPD\nnNwwPJ3560ooLA2+FyIopVRjBMUwDGdyw/B0nvtsMy8v3srvLg+cM65SqgE+eAh2F/h2m0lZcMmT\np1z80EMPkZaWxp133gnAI488gsvlYsGCBezbt4/q6moee+wxJk6c2KjdVlZWcvvtt5Obm4vL5eLp\np59mzJgxrFmzhltuuYWqqiq8Xi//+te/6NKlC9dccw1FRUV4PB5+9atfMWnSpGYdNoTAlT5AQnQ4\nlw5M5t8rijlSVePvcJRSAW7SpEnMnj37+Pzs2bOZMmUK//73v8nLy2PBggXcf//9NHYYm2eeeQYR\noaCggDfffJMpU6ZQWVnJ888/zz333MPKlSvJzc0lNTWVefPm0aVLF/Lz81m9ejXjx4/3ybGd8Upf\nRF4CLgVKjDED7LI4YBaQAWwFrjHG7BPr1ezTgQnAEeBmY0yevc4U4GF7s48ZY171yRE00LVDuvJ2\nXjHvrdrF1TlprblrpVRznOaKvKUMGjSIkpISdu7cSWlpKbGxsSQlJXHvvfeycOFCHA4HxcXF7Nmz\nh6SkpAZvd9GiRdx1110A9OnTh/T0dDZs2MCIESN4/PHHKSoq4gc/+AGZmZlkZWVx//338+CDD3Lp\npZcyatQonxxbQ670XwFOPsU8BMw3xmQC8+15gEuATPszFXgOjp8kfgMMA4YCvxGR2OYG3xhDMmLp\n3qk9s3N3tOZulVJt1NVXX82cOXOYNWsWkyZN4vXXX6e0tJTly5ezcuVKEhMTqays9Mm+rr/+eubO\nnUtkZCQTJkzg008/pVevXuTl5ZGVlcXDDz/Mb3/7W5/s64xJ3xizEDi5v+NEoPZK/VXg8jrlrxnL\nV0CMiCQDFwMfG2P2GmP2AR/z3RNJixIRrs5JY9nWfWwv16EZlFKnN2nSJN566y3mzJnD1Vdfzf79\n++ncuTNut5sFCxawbVvj390xatQoXn/9dQA2bNjA9u3b6d27N4WFhXTv3p27776biRMnsmrVKnbu\n3Em7du244YYbeOCBB3w2Nn9T2/QTjTG77OndQO3g9SlA3UvpIrvsVOWt6vsDkwF4r2DXGWoqpUJd\n//79OXjwICkpKSQnJzN58mRyc3PJysritddeo0+fPo3e5h133IHX6yUrK4tJkybxyiuvEB4ezuzZ\nsxkwYADZ2dmsXr2am266iYKCAoYOHUp2djaPPvooDz/88Jl30AANGk9fRDKAd+u06VcYY2LqLN9n\njIkVkXeBJ40xi+zy+cCDwPlAhDHmMbv8V8BRY8wf69nXVKymIbp27Xp2U86mpzPxmcV4vF7evcs3\n7WNKKd/T8fQbrrXG099jN9tgf5fY5cVA3bukqXbZqcq/wxgz0xiTY4zJSUhIaGJ4p3ZpVjKriw+w\nrfywz7etlFKBrqlJfy4wxZ6eArxTp/wmsQwH9tvNQB8C40Qk1r6BO84ua3WXZFl32rWJRynlSwUF\nBWRnZ3/rM2zYMH+H9R0N6bL5JlbzTCcRKcLqhfMkMFtEbgW2AdfY1d/H6q65CavL5i0Axpi9IvI7\nYJld77fGGL8MhpMa247stBjeW7WLO87v6Y8QlFINYIzB6gXeNmRlZbFy5cpW3WdTXnd7xqRvjLnu\nFIsurKeuAe48xXZeAl5qVHQt5JIBSTzxwTp2VhylS0ykv8NRSp0kIiKC8vJy4uPj21Tib03GGMrL\ny4mIiGjUeiExDMPJxvTpzBMfrGPhhlKuHdrV3+EopU6SmppKUVERpaWl/g4loEVERJCamtqodUIy\n6Wd2jiK5YwSfrdekr1QgcrvddOvWzd9hBKWQGHvnZCLCeb0SWLypjGqP19/hKKVUqwnJpA9wfu8E\nDh6rIW/bPn+HopRSrSZkk/45PTvhcgifbdA2Q6VU6AjZpN8hws3g9Fg+X69JXykVOkI26QOc27MT\na3cfoOJIlb9DUUqpVhHSSX9YtziMgWVbtV1fKRUaQjrpD0yLIczlYGlhub9DUUqpVhHSST/C7SQ7\nLYalW/wyIoRSSrW6kE76AMO7xbFm534OVlb7OxSllGpxIZ/0h3WPx2sgV/vrK6VCQMgn/UFdY3A5\nhK+1iUcpFQJCPum3C3NxVmpHvZmrlAoJIZ/0Ac5Oj2X1zgNU1eg4PEqp4KZJH8hOi6WqxsvaXQf8\nHYpSSrUoTfpAdlfrHe8rd1T4ORKllGpZmvSBLh0jSIgO16SvlAp6mvSxxtfPTovRpK+UCnqa9G3Z\naTFsKTusg68ppYKaJn3boDRt11dKBT9N+ras1I6IaNJXSgU3Tfq26Ag3mZ2jyNekr5QKYpr068hK\niWH1Tu2rr5QKXpr06+jfpQOlB49RcqDS36EopVSL0KRfR/8uHQBYo1f7Sqkg1aykLyL3isgaEVkt\nIm+KSISIdBORpSKySURmiUiYXTfcnt9kL8/wxQH4Ur/jSX+/nyNRSqmW0eSkLyIpwN1AjjFmAOAE\nrgV+D0wzxvQE9gG32qvcCuyzy6fZ9QJKdISbjPh2eqWvlApazW3ecQGRIuIC2gG7gAuAOfbyV4HL\n7emJ9jz28gtFRJq5f5/rn9KR1Xqlr5QKUk1O+saYYuCPwHasZL8fWA5UGGNq7GpFQIo9nQLssNet\nsevHN3X/LaV/lw7s2HuU/Uf19YlKqeDTnOadWKyr925AF6A9ML65AYnIVBHJFZHc0tLS5m6u0fp3\n6QjAN9rEo5QKQs1p3rkI2GKMKTXGVANvAyOBGLu5ByAVKLani4E0AHt5R+A7r6syxsw0xuQYY3IS\nEhKaEV7T9NebuUqpINacpL8dGC4i7ey2+QuBb4AFwFV2nSnAO/b0XHsee/mnxhjTjP23iE5R4SR1\niNCbuUqpoNScNv2lWDdk84ACe1szgQeB+0RkE1ab/Yv2Ki8C8Xb5fcBDzYi7RfVJjmb97oP+DkMp\npXzOdeYqp2aM+Q3wm5OKC4Gh9dStBK5uzv5aS+/EaL7cXE6Nx4vLqc+vKaWCh2a0evROiqaqxsvW\n8sP+DkUppXxKk349eidFA7BOm3iUUkFGk349eiRE4XSItusrpYJOYCf9wyWwt7DVdxvhdpIR306v\n9JVSQadZN3Jb3P5i+MsgSOgD/X8Ag2+EDl1aZdd9kjrocAxKqaAT2Ff6if1h/O+hfQJ89v9g2gB4\neyqUb27xXfdOimb73iMcqao5c2WllGojAjvpO8Ng+I/h5nfh7hUw/Hb4Zi7MGAIfPATHDrXYrnsl\nRmMMbNjTcvtQSqnWFthJv6647nDx4/CTVXD2FFj6HDw7ArYvbZHd9bF78KzfrU/mKqWCR9tJ+rWi\nOsOl0+CWeeBwwivfg+Wv+Hw3XePaEeF26M1cpVRQaXtJv1b6CJi6ALqfB/+9Bz7z7TtZHA6hV2I0\nG/Zo0ldKBY+2m/QBImPh+tmQPdm60fv5Uz7dfO9EHYNHKRVc2nbSB6uJ57K/wsDrYcHjkPuyzzbd\nOymaskNVlB065rNtKqWUP7X9pA9W4p84A3peBO//FLYt8clma4dj2KBX+0qpIBEcSR+sxH/li9Ax\nDd6+DY5WNHuTvRPtpK/t+kqpIBE8SR8gMgaufAEO7IQPHmz25hKiw+kY6Wa99tVXSgWJ4Er6AKk5\nMOp+WPUWFH7WrE2JCL0So9ioV/pKqSARfEkfrKQf1x3evQ9qqpq1qdpumwH4ZkellGq04Ez67ggY\n/yTs3QwrXmvWpnolRnOgsoY9B7QHj1Kq7QvOpA+QOQ66jrD67lcdafJmeunNXKVUEAnepC8CF/4G\nDu2Bpc83eTO9EqMATfpKqeAQvEkfrKEaMi+GxdOhqmnvu42PCie+fZgmfaVUUAjupA9w7r1QWQH5\nbzV5E9bNXO22qZRq+4I/6XcdDsnZVhOP19ukTdR229QePEqpti74k74IDL8DyjbA5k+btInMxGgO\nV3korjjq4+CUUqp1BX/SB+h/BUQlWi9eaYLaMXg2ahOPUqqNC42k7wqDwTfBpvnWy9YbqVdn+y1a\nejNXKdXGNSvpi0iMiMwRkXUislZERohInIh8LCIb7e9Yu66IyF9EZJOIrBKRwb45hAYaeB1gYNWs\nRq/asZ2bxA7h2oNHKdXmNfdKfzowzxjTBxgIrAUeAuYbYzKB+fY8wCVApv2ZCjStraWp4ntA2nDI\nfxOacENW36KllAoGTU76ItIRGA28CGCMqTLGVAATgVftaq8Cl9vTE4HXjOUrIEZEkpsceVNkX2fd\n0C3Oa/SqmZ2j2VRyCK9Xe/Aopdqu5lzpdwNKgZdFZIWIvCAi7YFEY8wuu85uINGeTgF21Fm/yC5r\nPf2vAFcE5L/R6FV7J0VRWe1lx76mD+mglFL+1pyk7wIGA88ZYwYBhznRlAOAsTq2N+rSWESmikiu\niOSWlpY2I7x6RHSEXuPhm7ng9TRq1Ux7DB59Z65Sqi1rTtIvAoqMMUvt+TlYJ4E9tc029neJvbwY\nSKuzfqpd9i3GmJnGmBxjTE5CQkIzwjuFfpfB4RLY8XWjVsvsbI3Bs7FEu20qpdquJid9Y8xuYIeI\n9LaLLgS+AeYCU+yyKcA79vRc4Ca7F89wYH+dZqDWkzkOnGGw9r+NWi06wk1KTKTezFVKtWmuZq5/\nF/C6iIQBhcAtWCeS2SJyK7ANuMau+z4wAdgEHLHrtr7waOhxgZX0L37cemK3gTITo7R5RynVpjUr\n6RtjVgI59Sy6sJ66BrizOfvzmb7fhw3zYNdK6DKowav1Tozmy03l1Hi8uJyh8VybUiq4hGbm6nUJ\niLPRTTyZidFUebxsLdcePEqptik0k377eGv0zQ0fNWq12heq6IvSlVJtVWgmfYCeF8GeAjjQ8HvJ\nPTtHIYKOra+UarNCN+lnjrW+N89v8CrtwlykxbbTHjxKqTYrdJN+4gCIToaNHzdqNR2DRynVloVu\n0heBnhdC4QLw1DR4tV6JUWwpO0xVTdPewqWUUv4UukkfoOdYqNwPxbkNXqVXYjQ1XsOWsqa9aF0p\npfwptJN+9/OtrpubPmnwKn2SrTF41u460DIxKaVUCwrtpB8ZYz2ctWVhg1fpmRBFhNvBqqL9LRiY\nUkq1jNBO+gDdRkHxcjjWsG6YLqeDvskdWF3cwKR/dJ/1QvYqfaBLKeV/mvQzRoG3BnZ81eBVslI6\nsmbn/jO/UOVwOfxtNPz9CnhpHFQfbWawSinVPJr0uw4Hhxu2fNHgVbJSOnK4ykPhmW7mfvFHOLAT\nzrkbdhfAF083M1illGoeTfph7SE1B7Y2IumndgQ4fRNP1RHI+7v1tq5xv7MGeft6JlRprx+llP9o\n0geriWfnCqv7ZgM06GZu4QKoOgjZk635YbdDZQWsfdcHASulVNNo0gfrZq7xwrYlDareoJu56z+A\n8I6Qca4133UERHeBtXN9ELBSSjWNJn2A1KHgDG9UE89ZZ7qZu30JZIwEp9uadzisVzVu/BiO6TAO\nSin/0KQP4I6AtKGN6q8/wL6Zu7m0nq6eR/ZC+SZIHfLt8r6XgecYbGzckM5KKeUrmvRrZYyyetgc\n2dug6jkZcQB8vbWe+kX2sA4nJ/2uw6F9Aqx7vzmRKqVUk2nSr5UxEjCwvWH99TPi29E5OpylhfUl\n/WUgDkgZ/O1yh9Max3/zfPB6mh+zUko1kib9Wik5Vrv+tsUNqi4iDO0Wx9It5Viv/62j6GtI7G91\nBz1Zz4usp3SL83wQtFJKNY4m/VruCKu/fgOTPsCw7vHsOXCM7XvrDLHg9VoJ/eSmnVo9LrD+CtjU\nuHH8lVLKFzTp15V+DuzKh8qGjaA5rJvVrv9VYfmJwv3b4dgBSB5Y/0rt4qwTgt7MVUr5gSb9utJH\nWv31dyxtUPXMzlEkdYhg/tqSE4V7vrG+O/c/9Yo9x1oPgx0qbUawSinVeJr060obCg5Xo9r1x/ZL\n5IuNZVRW2zdm96yxvjv3PfWKmRdZ3414P69SSvmCJv26wtpDl8GwteHt+mP7JXK02sOijWVWQcka\niM2A8KhTr5Q0ENp31iYepVSr06R/svRzYGdegwdGG949nuhwFx+u2W0V7Flz+qYdsJ7O7XmRNc6+\ndt1USrWiZid9EXGKyAoRedee7yYiS0Vkk4jMEpEwuzzcnt9kL89o7r5bRMa59vj6XzeoepjLwbj+\nSbxfsItDhw9ZT+ImniHpg9XEc3Sf9QIXpZRqJb640r8HWFtn/vfANGNMT2AfcKtdfiuwzy6fZtcL\nPGnDrC6V275s8CqTh3flcJWHTxcutG4EJ/Y780q1XTe1iUcp1YqalfRFJBX4HvCCPS/ABcAcu8qr\nwOX29ER7Hnv5hXb9wBLRAZLOalR//cFdYxnWLY7ly+x1ztS8AxAZaw30tlH76yulWk9zr/T/DPwM\n8Nrz8UCFMabGni8CUuzpFGAHgL18v13/W0RkqojkikhuaamfujRmnGuNn1Nd2eBVfjGhLylVW6gW\nN57Ybg1bKfMi2LUSDpWcua5SSvlAk5O+iFwKlBhjfNoobYyZaYzJMcbkJCQk+HLTDZc+0hoNsxHt\n7QPTYhjfeS8bPF24aubXPP3xBl74opCXFm3hwzW7OVhZ/d2VMsdZ35s+8VHgSil1eq5mrDsSuExE\nJgARQAdgOhAjIi77aj4VKLbrFwNpQJGIuICOQPl3NxsA0kcAYjXxZIxs8Gpp1dvYlnY2+yqq+Oun\nG6k7JE+7MCd3nN+DqaN7EOayz7VJZ0FUotXEk329b49BKaXq0eSkb4z5OfBzABE5H/ipMWayiPwT\nuAp4C5gCvGOvMteeX2Iv/9R8Z6SyABEZa/XA2boIzvtZw9Y5ug85uJOM4T/ms5FjqPZ4OVLlwRjD\nut0HeWXxVv740QYWbizjucmDiY8KBxGr6+a6d6GmClxhLXtcSqmQ1xL99B8E7hORTVht9i/a5S8C\n8Xb5fcBDLbBv30kfaXXbrKlqWP3jwy9YPXfcTgcdI93EtAtjePd4nr/xbKZfm03+jgoum7GYtbvs\n8X36TbTezasDsCmlWoFPkr4x5jNjzKX2dKExZqgxpqcx5mpjzDG7vNKe72kvL/TFvltMxkioOWrd\naG2Ikm8n/fpMzE7hnz8eQY3Xy5XPfcnH3+yBHhdaT+eufMMHQSul1OnpE7mn0vUc63vroobVL/nG\nehF6hy6nrXZWagxz//dcenaOYurfc5m+YAuerKthw4cNfmuXUko1lSb9U4lKgE69G95fv2St9VBW\nAx49SOwQwaypI7hsYBemfbKBO9b0BW81JvflZgatlFKnp0n/dDJGwval4Kk5fT1jrDb90zTtnCwy\nzMn0awcx88azWVXVhc89Z1Gx4C/MWrKRfYcbeB9BKaUaSZP+6aSPhKqDsHvV6esdKIZj+08/nPIp\njOufxIKfnk/NOfcQayooePcZhjz+CTe//DVzlhex/2g9/fuVUqqJmtNPP/il2330ty3+7kvO69q5\nwvruMqhJu4lwO7lw/JWYXS/wSMk7xPe/ljlrDvHTf+bjdgqjMhOYkJXM2L6JdGznbtI+lFIKNOmf\nXodkiOtuja9/zl2nrle8HBxuSBzQ9H2JIBP+gOtvo7nX+w9+8uBfWbGjgg8KdvF+wW4+XVeC2ymM\n7NmJCVnJjOuXSEw77devlGocTfpnkj4S1s61XnjuOEVrWPFySBpgvVy9OZKyrJPL4ulIag6Dz76Z\nwV1j+cWEvuQX7eeDgl28V7CLn81ZxS8cwjk9O/G9rCTG9Usitr2eAJRSZ6ZJ/0zSR8KKv1tvxErK\n+u5yrxeKV8DASb7Z3wW/ht2r4d37oOYYDJ2KiJCdFkN2WgwPXdKHguL9vF+wm2X5BSz893/Z9U4x\nfWK8pCbEkpjRl079zkcSevkmHqVUUNGkfya1Y+9sWVh/0i/faN3sTTnbN/tzuuCaV+Fft8EHP4NV\ns2DgdRDfE2oqkdL1nLV7FWft+Boqd4B9gX/4UCRhB6twb/HAAtgZ0ZPSzEkkj7mNznGxvolNKdXm\nadI/k5iu1vj4a9+FEXd+d3nRMuu7y2lu9DZWeDRc+wbkvQKLp8P7P/328g4pkDrEiidtKCRm0c7p\nZlvZIfJXraBq7Tz6ls1jYMHjlKx6hmcjr2Jv3xsYntmFYd3jiI7Qm8FKhSoJ1DHPAHJyckxubq6/\nw4DPfg+fPQH3r4PopG8ve/t/rKGRf7rx1G3+zWEM7N8BFTvAGQadeloDwp2B12vYmvcR4YueIqUi\nl2LTiWk1V/KOGU1WaiwjesQzonsnzk6PJTLM6fu4lVJ+IyLLjTE59S7TpN8AJevg2WEw4Y8w9LYT\n5cbAn/pYTUBXveS/+M5k8wK8nzyKY9cKyiK78WLYZP6vtB81XnA7rfsFI7rHM7xHPIO7xhLh1pOA\nUm2ZJn1fmDEU2sXBD+edKCvOg/8bAxOfhUGT/RdbQxhj9UKa/zso34gneTAFfe9l3uFeLCksp6Co\nAq+xXvQ+KC3G/ksgnuyuMYS79CSgVFuiSd8XFk+Hj38Nty858eLzjx6Gr56HBzY2qMklIHhqIP9N\nq7nqQLH1QNnA6zmUeRlflwhLNpezpLCcNTsPYAyEuxycnR7LiO7xjOgRz1mpMSdeAqOUCkia9H3h\ncDn8OQt6j7eacqqOwLT+kH4OXPu6v6NrvOpKyHvV6o66uwDEYfVA6nEBpI9kf1wWX++sPn4SqB3/\nP9LtJCcjluG1J4GUjricehJQKpBo0veV+b+FL/4E174JO5bC4j/DLfPs1yu2YbtXwzfvQOEC60Ez\n4wUEOvWyTgQpgzkQP5Alh5NZsvUASzaXs37PQQDahzkZ0i2O0ZkJXNCnMxmd2vv3WJRSmvR9pvoo\nvDjWujIGGHQDTHzGvzH52tF9VuIvzrO+i3LhSJm1LCwKug6HjHPZnziMxUe7sqSwgsWbyygsPQxA\n907tGdOnM2N6d2ZotzhtClLKDzTp+9Kxg7D8FasvffYN1sNUway2y2jRMtj2pfVSmdJ11rL2CdBr\nPPT5HjtihjJ/0wEWrC9lSWE5VTVe2oc5OTezExf3T+LCvol0jNTnA5RqDZr0lW8dKoUtn8O696xn\nFI4dAHc7635A38s40m0sXxZV8+n6Ej5dW8LuA5W4ncK5PTtxyYBkxvZL1LGClGpBmvRVy6mpgq1f\nwPr3Yd37cHCn9RBZ9zHQbyLeXhPIL4cPVu/m/YJdFO07itMhnNMjnh8MTuHi/km0Cwvyv5aUamWa\n9FXr8HqhONe6KfzNO1azkMMF3c+HfhMxvSewep+b91fv4r/5Oynad5T2YU4uyUrmysGpDOsWh8Nx\n5tdNKqVOT5O+an3GwM48Kwq/0uEAAA9mSURBVPmv+Q9UbANxQrdR1l8AvS9lWamTf+UV8X7Bbg4d\nqyElJpKrzk7luqFdSerYzGGqlQphmvSVfxkDu/LtvwD+A3sLrecC0kdCv4kc7TmBj7bDnOVFLNpU\nhkOEcf0SuXF4OiN6xCMNeNm8UuoETfoqcBgDe9acOAGUbQDEOgEMmsz2pLG8nlfGrNwdVByppkdC\ne24akcHVOana9q9UA2nSV4GrZJ2V/FfNsv4CCIuC/ldw7Kzrebc8jdeWbid/RwUx7dzcNCKDKSPS\niY8K93fUSgU0Tfoq8BkD27+CFf+ANf+G6sMQnwmDbiA/bjx/XXaIT9buIdzl4JqcNH58fg9SYiL9\nHbVSAalFkr6IpAGvAYmAAWYaY6aLSBwwC8gAtgLXGGP2idUwOx2YABwBbjbG5J1uH5r0Q9Sxg9bN\n3xX/gB1fWTeAM8exq8dVTN/ejX+t3IMgXDs0jTvH9CSxg970Vaqulkr6yUCyMSZPRKKB5cDlwM3A\nXmPMkyLyEBBrjHlQRCYAd2El/WHAdGPMsNPtQ5O+omyjlfzz34JDuyE6mQP9ruXZ/SN5YVU1Todw\nw/B0bj+/B5202UcpoJWad0TkHWCG/TnfGLPLPjF8ZozpLSJ/s6fftOuvr613qm1q0lfHeWpg44eQ\n+7L1FLDDycH+N/LHYxP5e8ER2oW5uGNMD344spu+BEaFvNMlfZ+MhiUiGcAgYCmQWCeR78Zq/gFI\nAXbUWa3ILjt5W1NFJFdEcktLS30RngoGThf0+R7cMAfuyYfBNxG9+jUe3TKZ3HNzOb9bO56at56L\nnv6c91btIpDvVSnlT81O+iISBfwL+Ikx5kDdZcb6n9eo/33GmJnGmBxjTE5CQkJzw1PBKDYdLp0G\n/7sMMscSt+xpZpTdyifnbaVDmHDnG3lc87clfLPzwJm3pVSIaVbSFxE3VsJ/3Rjztl28x27WqW33\nL7HLi4G0Oqun2mVKNU18D7jmVbj1Y4hJp+fSX/Be+C94edQBCksP8/0Zi3ji/bUcqarxd6RKBYwm\nJ327N86LwFpjzNN1Fs0FptjTU4B36pTfJJbhwP7Ttecr1WBpQ+HWj+DqV5HqI4xZ9mO+SpvB3f0q\n+dvCQsY+vZBFG8v8HaVSAaE5vXfOBb4ACgCvXfwLrHb92UBXYBtWl8299kliBjAeq8vmLcaY096l\n1Ru5qtFqjsGyF+Dzp+DYQXb2v40fbr2AdWU13HxOBg+O70NkmN7oVcFNH85SoefoPvjoV7Di73jj\nevJq/L08WhBL94T2TLsmm4FpMf6OUKkW0+K9d5QKOJGxMHEG3PgfHN5qbtl4J18NeAfHsQP84Lkv\nef7zzdrDR4UkTfoquPUYA3csgXPuJmnzHD6KfJj/6bGXJz9Yx22vLWf/kWp/R6hUq9Kkr4JfWHsY\n9zv44TwcGB4ovod/Zn3N5+t3c+mMLygo2u/vCJVqNZr0VehIGwo/Xoj0nsCQjX8mt9vf6FCzjyuf\n+5J/5u448/pKBQFN+iq0RMbCNa/B956m4+6l/Nf9EDcnb+GBOat4ZO4aqj3eM29DqTZMk74KPSIw\n5FaYugBHZCw/L/sF/8j4kH98uYkbX1xK+aFj/o5QqRajSV+FrsT+MHUBMvhGzt39KkuTn2b39o1c\nNmMxa3ZqO78KTpr0VWgLaw+X/RWufJH4w5v5pN0vGV3zJVc+9yVz83f6OzqlfE6TvlIAWVfBjxfi\n6tSTJ2r+wPSov/PAm0t54oO1eLzan18FD036StWK6w4//BDOuYuLj77H5zG/45OFX3DLK8vYf1T7\n86vgoElfqbpcYTDuMZg8hyTHfj6M/BUphf/kihmL2Fx6yN/RKdVsmvSVqk/mWLh9Ma70oTzhmsld\nR/7KVTM+Z8H6kjOvq1QA06Sv1KlEJ8GN/4FR93OFmc8r7ie595UFzFyo4/aotkuTvlKn43DChb+G\ny5/nLO9aPoz6HW98sID7ZudTWe3xd3RKNZomfaUaIvs6ZMpcOrsO80H737Jz5SdMmvkVew5U+jsy\npRpFk75SDZV+DnLbfCJjEnkz4kkG7JnL9/+6iNyte/0dmVINpklfqcaI6w63foyj27k87nien/AP\nrp35Jc8s2IRX+/OrNkCTvlKNFRkDk/8JOT/k+up/MyfueWZ8mM+Ul7+m9KCO26MCmyZ9pZrC6Ybv\nPQ3jn2Tg4cUs7vwHdm5Zy4S/fMH8tXv8HZ1Sp6RJX6mmEoHhtyPXvUXcsWI+jvg5Nzrnc+ury7hv\n1koqjlT5O0KlvkOTvlLN1etiuP1LHF2Hcnfls3yW9BeW5RcwdtpC3llZrH36VUDRpK+UL8SkWQ9y\nfe9PZBxezWftf84NYZ/zk7fy+MFzX7J82z5/R6gUoElfKd8RgSE/gtsX4UzO4p7Df2FZ8h+J3LuW\nK5/7khteWMqXm8v0yl/5lQTyL2BOTo7Jzc31dxhKNZ7XC/lvwMe/xhzdx/qky/hp6SWsPhRNr8Qo\nrjo7lcuzU+jcIcLfkaogJCLLjTE59S7TpK9UCzqyFz5/CnJfxCCs7zqJpw6M49Ni64/sASkdGJ2Z\nwJBucQzo0pGE6HA/B6yCgSZ9pfytYjt89nvr6l8cHMoYxxfho3mzvBeLi44df1FLQnQ4abGRdImJ\nJK59GJFuJ+FuJ26H4HI6cDsFt9OByym4Hda3y+k4vry23O08Ud9VZ97lqLP+SctFxM8/JOUrAZX0\nRWQ8MB1wAi8YY548VV1N+irolG+G5a/AytfhSDk43HgSB1DaYQDrnL3Ir0xk9dE4NhxwceBoNUer\nPVRWe5u8Owde3NTgxIsLDw68uPDixIMLD3XzvEMEtwNcTgdOBydOFg4HOJ04HG5wuhCnC4fDhbhc\nOJ1uHE63fYKxT0LHTzInpq1ljm+dvFyOk05MrvpPXm6Xg7A6J6owe191l7mdDpwOPWnVCpikLyJO\nYAMwFigClgHXGWO+qa++Jn0VtDw1UPQ1bPwIinKhOA+qD59YHhEDsekQFoVxujE4MJ5qTE0VxlMN\n3mrwVIGnGrw11renGvFWI94a+7saoXX+f3tw2B+n/XFQg5MaHNQYe95Yy2vsel4EL4LBcXzaiwOD\n4DUnzdvTVv0T09466xocIA5EHOA4MV07Lw573uFExIFTwCkGl8PgEoNTDE7sbwGneK1vvMeXOcTg\nsOs47MidYkdpapfbURrv8b+eRASxJuxvByLUWe44sVxAHC4QJzhc1rE4XNbnlGW1H6tMLvjlKZO+\nqzV+IeoYCmwyxhQCiMhbwESg3qSvVNByuiD9HOsD4PVA2QYo3wR7t8C+LVCxA6qPIlVHEOMFZxiE\nhYMjynoi2OkGh7ue6TDrP//xMpf1fTwpOE8kC6mnA9/JzTzGgPFYJxdv7Xfdjwentwan91R1rHnj\nrcF4rI/XW40xBuP1Yrwe69t4T3wbD9jLMFYZ3hrr+/i8NW19DGBNi/3BWKcDjEFq7FODOXEKsU4u\n3z7heBE8dcvtk4/HXlZz0knm5HXrnpgMgjGCiHXiPfkEXDt/4vtEubP2LzKx/kKzTkaeE9P2abP2\n2/Gtv+ROP+R3ayf9FGBHnfkiYFjdCiIyFZgK0LVr19aLTCl/cjihc1/rE6SEE4nN6c9AGsHrNVR7\nvVR7DNU1Xqo9Xqo89rzHS1WNF0/deY+X6hovHq/BYwwer8FrDB6vta1vl520/DtlVn2v96Tlx8sM\nBqvcGOvba8Dr9QL1XuQDrZ/0z8gYMxOYCVbzjp/DUUqFMIdDCHc4CXcBbahj1bM3nHpZaz+cVQyk\n1ZlPtcuUUkq1gtZO+suATBHpJiJhwLXA3FaOQSmlQlarNu8YY2pE5H+BD7Ga9V4yxqxpzRiUUiqU\ntXqbvjHmfeD91t6vUkopHXBNKaVCiiZ9pZQKIZr0lVIqhGjSV0qpEBLQo2yKyEFgvb/j8KNOQJm/\ng/CjUD7+UD520ONv7vGnG2MS6lsQcE/knmT9qQYNCgUikqvHH5rHH8rHDnr8LXn82ryjlFIhRJO+\nUkqFkEBP+jP9HYCf6fGHrlA+dtDjb7HjD+gbuUoppXwr0K/0lVJK+ZAmfaWUCiEBm/RFZLyIrBeR\nTSLykL/jaQki8pKIlIjI6jplcSLysYhstL9j7XIRkb/YP49VIjLYf5E3n4ikicgCEflGRNaIyD12\neagcf4SIfC0i+fbxP2qXdxORpfZxzrKHIEdEwu35TfbyDH/G7wsi4hSRFSLyrj0fSse+VUQKRGSl\niOTaZa3yux+QSd9+gfozwCVAP+A6Eenn36haxCvA+JPKHgLmG2Mygfn2PFg/i0z7MxV4rpVibCk1\nwP3GmH7AcOBO+984VI7/GHCBMWYgkA2MF5HhwO+BacaYnsA+4Fa7/q3APrt8ml2vrbsHWFtnPpSO\nHWCMMSa7Tn/81vndN8YE3AcYAXxYZ/7nwM/9HVcLHWsGsLrO/Hog2Z5OxnpADeBvwHX11QuGD/AO\nMDYUjx9oB+RhvS+6DHDZ5cf/H2C9g2KEPe2y64m/Y2/GMafaie0C4F2s1+eGxLHbx7EV6HRSWav8\n7gfklT71v0A9xU+xtLZEY8wue3o3kGhPB+3PxP5zfRCwlBA6frt5YyVQAnwMbAYqjDE1dpW6x3j8\n+O3l+4H41o3Yp/4M/Azw2vPxhM6xAxjgIxFZLiJT7bJW+d0P9GEYQpoxxohIUPepFZEo4F/AT4wx\nB0Tk+LJgP35jjAfIFpEY4N9AHz+H1CpE5FKgxBizXETO93c8fnKuMaZYRDoDH4vIuroLW/J3P1Cv\n9EP5Bep7RCQZwP4uscuD7mciIm6shP+6MeZtuzhkjr+WMaYCWIDVpBEjIrUXY3WP8fjx28s7AuWt\nHKqvjAQuE5GtwFtYTTzTCY1jB8AYU2x/l2Cd8IfSSr/7gZr0Q/kF6nOBKfb0FKy27trym+w7+cOB\n/XX+FGxzxLqkfxFYa4x5us6iUDn+BPsKHxGJxLqfsRYr+V9lVzv5+Gt/LlcBnxq7gbetMcb83BiT\naozJwPq//akxZjIhcOwAItJeRKJrp4FxwGpa63ff3zc0TnOjYwKwAaud85f+jqeFjvFNYBdQjdVO\ndytWW+V8YCPwCRBn1xWsHk2bgQIgx9/xN/PYz8Vq11wFrLQ/E0Lo+M8CVtjHvxr4tV3eHfga2AT8\nEwi3yyPs+U328u7+PgYf/RzOB94NpWO3jzPf/qypzW+t9buvwzAopVQICdTmHaWUUi1Ak75SSoUQ\nTfpKKRVCNOkrpVQI0aSvlFIhRJO+UkqFEE36SikVQv4/kcep7RJkPbkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRzPRFI9lHXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ked0qqF8lHXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFISi_-UlHX4",
        "colab_type": "code",
        "outputId": "5d625677-81d5-40e8-f8b1-932aa63c6a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "current_mse=mean_absolute_error(y_test,predictions)\n",
        "current_mse"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8050009608268738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO2gcj28lHYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_with_best_model(curr_model,X_test,y_test):\n",
        "    best_model=None\n",
        "    try:\n",
        "        best_model=tf.keras.models.load_model('./sample_data/best_model.h5') \n",
        "        best_pred=best_model.predict(X_test)\n",
        "        best_mse=mean_absolute_error(y_test,best_pred)\n",
        "        \n",
        "        curr_pred= curr_model.predict(X_test)\n",
        "        curr_mse= mean_absolute_error(y_test,curr_pred)\n",
        "        if curr_mse<best_mse:\n",
        "            curr_model.save('./sample_data/best_model.h5')\n",
        "            best_model=curr_model\n",
        "            print(f\"New best model with mse {curr_mse}\")\n",
        "        else:\n",
        "            print(f\"No new best model, still best mse {best_mse}\")\n",
        "    except:\n",
        "        print(\"No best model found\")\n",
        "        best_model=curr_model\n",
        "        \n",
        "    return best_model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOgWvrublHYO",
        "colab_type": "code",
        "outputId": "df33ebeb-3e48-4878-f7e5-821add685acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model = compare_with_best_model(model,X_test,y_test)\n",
        " \n",
        "def predict_function(a,b,best_model,scaler):\n",
        "    tmp=[[a,b]]\n",
        "    tmp=scaler.transform(tmp)\n",
        "    pred=best_model.predict(tmp)\n",
        "    #tmpDF=pd.DataFrame({'A': [a],'B': [b]})\n",
        "    #tmpDF=scaler.transform(tmpDF)\n",
        "    #pred=best_model.predict(tmpDF)\n",
        "    return pred[0][0]\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
            "No new best model, still best mse 0.3222756087779999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iakjVqUFvIB",
        "colab_type": "code",
        "outputId": "5b386846-aef5-4228-a96f-5af8bf7bbe03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "# Our predictions\n",
        "plt.scatter(y_test,predictions)\n",
        "# Perfect predictions\n",
        "plt.plot(y_test,y_test,'r')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fddc536f400>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAJcRCSiFOAiIhH1f6VVNw/uJwM7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syT-YmR8lHYR",
        "colab_type": "text"
      },
      "source": [
        "## Let's test our best module with 5 multiplications it had never seen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUbpQvlSGVV-",
        "colab_type": "code",
        "outputId": "49fc47b4-c87e-4877-b89f-1dd9ef93cbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(\"8 * 2 = {}\".format(round(predict_function(8,2,model,scaler))))\n",
        "print(\"7 * 2 = {}\".format(round(predict_function(7,2,model,scaler))))\n",
        "print(\"5 * 6 = {}\".format(round(predict_function(5,6,model,scaler))))\n",
        "print(\"8 * 8 = {}\".format(round(predict_function(8,8,model,scaler))))\n",
        "print(\"8 * 5 = {}\".format(round(predict_function(8,5,model,scaler))))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 * 2 = 15.0\n",
            "7 * 2 = 13.0\n",
            "5 * 6 = 30.0\n",
            "8 * 8 = 62.0\n",
            "8 * 5 = 40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqS9pqIlHYW",
        "colab_type": "code",
        "outputId": "8b3bb78b-4162-4087-d9d1-a0641b16973c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_399 (Dense)            multiple                  6         \n",
            "_________________________________________________________________\n",
            "dense_400 (Dense)            multiple                  90        \n",
            "_________________________________________________________________\n",
            "dense_401 (Dense)            multiple                  15500     \n",
            "_________________________________________________________________\n",
            "dense_402 (Dense)            multiple                  15030     \n",
            "_________________________________________________________________\n",
            "dense_403 (Dense)            multiple                  31        \n",
            "=================================================================\n",
            "Total params: 30,657\n",
            "Trainable params: 30,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQcBKgSIlHYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPBQCqh_veqA",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz-Lf9n0lHYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}